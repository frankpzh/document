#+TITLE: NestCloud: Towards Practical Nested Virtualization

#+LaTeX_CLASS: ieee

#+STARTUP: showall
#+OPTIONS: author:nil
#+OPTIONS: toc:nil

#+LATEX_HEADER: \usepackage{cases}
#+LATEX_HEADER: \usepackage{graphicx}

#+LATEX_HEADER: \author{
#+LATEX_HEADER: \IEEEauthorblockN{Zhenhao Pan}
#+LATEX_HEADER: \IEEEauthorblockA{Tsinghua University,\\China\\
#+LATEX_HEADER: frankpzh@gmail.com}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \IEEEauthorblockN{Qing He}
#+LATEX_HEADER: \IEEEauthorblockA{Intel Asia-Pacific Research\\and Development Ltd\\
#+LATEX_HEADER: qing.he@intel.com}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \IEEEauthorblockN{Wei Jiang}
#+LATEX_HEADER: \IEEEauthorblockA{Tsinghua University,\\China\\
#+LATEX_HEADER: jwhust@gmail.com}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \IEEEauthorblockN{Yu Chen}
#+LATEX_HEADER: \IEEEauthorblockA{Tsinghua University,\\China\\
#+LATEX_HEADER: yuchen@tsinghua.edu.cn}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \IEEEauthorblockN{Yaozu Dong}
#+LATEX_HEADER: \IEEEauthorblockA{Intel Asia-Pacific Research\\and Development Ltd\\
#+LATEX_HEADER: eddie.dong@intel.com}
#+LATEX_HEADER: }

#+LATEX: \begin{abstract}

This paper describes a nested virtualization solution, which allows
virtual machine monitor (VMM) with virtual machine to run within
another virtual machine with low overhead. Previous nested
virtualization solutions on x86 platform are mainly based on
emulation, which result in poor performance and poor usability. We
propose and implement =NestCloud=, a practical high performance nested
virtualization architecture, which fully employs the hardware
virtualization extensions. Furthermore, three optimizations are
provided to reduce the overhead of nested guests: (1) Guest Page Fault
Bypassing, which permits nested guests to handle page faults without
VM Exit; (2) Virtual EPT (Extended Page Table), which eliminates
unnecessary page faults introduced by shadow page table in nested VMM;
(3) PV VMCS, which provides more effective VMCS accessing for nested
VMM. Experimental results show that the performance of NestCloud guest
is close to single level guest in both CPU-intensive and
memory-intensive benchmarks. The CPU overhead is 5.22\% and the memory
overhead is 5.69\%, which makes the nested guest of NestCloud
comparable with a conventional one.
#+LATEX: \end{abstract}

#+LATEX: \begin{IEEEkeywords}

Nested Virtualization, Virtual Machine Monitor (VMM), Virtual-Machine
Control Structure (VMCS)
#+LATEX: \end{IEEEkeywords}

* Introduction
  Virtualization has been widely used nowadays. In data centers and
  cloud computing environments, virtualization can largely reduce the
  hardware costs and resource
  costs\cite{survey-vm,view-cloud,berkeley-cloud}. There are
  commercial VMM (Virtual Machine Monitor) implementations such as
  VMware\cite{vmware} and Microsoft Hyper-V\cite{hyper-v}, and open
  source implementations such as Xen\cite{xen-art},
  KVM\cite{kvm,kvm-paper}, VirtualBox\cite{vbox} and
  lguest\cite{lguest}. Para-virtualization and full virtualization are
  two common virtualization techniques. Para-virtualization modifies
  the guest OS (operating system) to provide virtualization on legacy
  processors.  Full virtualization, on the other hand, virtualizes the
  guest OS without any modification\cite{intel-vt}.

  Nested virtualization, which is also known as recursive
  virtualization\cite{recur-vm}, allows one VMM to run within a
  virtual machine provided by another VMM. Although nested
  virtualization has not been widely used, we can still list several
  important usage models. We believe some of them have great potential
  in the future.

  * Some latest OS features and applications are necessary to run in
    virtualization environments. Windows XP mode\cite{xp-mode} is an
    example, which runs traditional Windows XP upon Windows 7 by
    virtualization. It is impossible to run Windows XP mode when the
    Windows 7 is already running in a virtual machine.

  * Recently, embedded virtualization technology (A.K.A hypervisor in
    firmware) has been adopted in some servers, which means the booted
    OS is already in a virtual machine. Nested virtualization can
    enable the traditional VMM working normally.

  * With help of nested virtualization, it is easy and efficient to
    debug and monitor guest OS upon a VMM, and even VMM itself.

  * For future cloud computing environment, the different
    virtualization solutions may vary much from each other, like
    diverse OS currently. Guest OS images for different VMMs may not
    be able to run on or live migrate\cite{lm} between different
    VMMs. The nested virtualization is a solution to this problem.

  To make virtualization much easier and faster, hardware vendors like
  Intel and AMD have added extensions to x86
  architecture\cite{intel-vt,amd-v}. Previous
  researches\cite{measure-cpu-io-xen,opt-net,bridge-gap-sw-hw,compare-vt}
  show that virtualization can achieve very high performance with
  these extensions. But on nested virtualization, current solutions
  have far worse performance than conventional
  virtualization. Previous studies of nested virtualization was
  designed using micro-kernels\cite{micro-vm} or on special
  virtualizable hardware architecture\cite{recur-vm}. Among the recent
  virtualization implementations, only KVM can support nested
  virtualization. Furthermore, KVM only supports AMD-V in nested
  virtualization\cite{kvm-nested}. On Intel processor, KVM can only
  use QEMU\cite{qemu} emulation, which has a very low performance and
  is not practical in reality.

  Based on the hardware extensions for virtualization, this paper
  proposes a new nested virtualization architecture called
  NestCloud. NestCloud uses the VMX instructions as interfaces, which
  is general and easy enough to apply to most VMMs on x86
  platform. Benchmark results indicate that NestCloud only introduces
  5.22% CPU overhead and 5.69% memory overhead.

  The remainder of this paper is organized as follows: Section
  \ref{sec-2} introduces the hardware extensions for virtualization
  (VMX) and KVM. Section \ref{sec-3} gives a description to the
  architecture design of NestCloud. Section \ref{sec-4} explains the
  implementation of NestCloud. Section \ref{sec-5} discusses three
  optimizations to the NestCloud. Section \ref{sec-6} uses well-known
  benchmarks such as SPEC CPU 2006, kernel build and SysBench to
  evaluate NestCloud's performance. At last, Section \ref{sec-7} is
  the related work and Section \ref{sec-8} is the conclusion and
  future work.

* Background
  In this section, we introduce the background of Intel's hardware
  extensions for virtualization and KVM (Kernel-based Virtual Machine).

** Hardware Extensions for virtualization
   The classic x86 architecture is not virtualizable according to
   Popek and Goldberg's virtualization
   requirements\cite{popek}. Current implementations of virtualization
   on x86 need either patches on the guest kernel, or hardware changes
   such as Intel VT\cite{intel-vt} and
   AMD-V\cite{amd-v}. Xen\cite{xen-art} is an example of the formal
   one, and KVM\cite{kvm} is an example of the latter one. NestCloud
   is based on Intel VMX extension\cite{intel-vt}.

#+CAPTION: VMX instruction, interaction of VMM and Guest
#+LABEL: fig:vmx
[[./nested2.eps]]

   \figurename \ref{fig:vmx} represents the instructions of VMX and
   how VMM and guests interact with each other. In terms of VMX, two
   operation modes are provided. Root operation mode is fully
   privileged and used by VMM. On the other hand, non-root operation
   mode is not fully privileged and used by guest OS. Software can
   enter VMX non-root operation mode using VM Entry instruction
   (VMLAUNCH or VMRESUME). In contrast, VM Exit is triggered by
   certain instructions and events in VMX non-root operation mode, and
   leads the processor to root operation mode.

   VMX contains a structure called VMCS (Virtual-Machine Control
   Structure). Each logical processor associates a memory region for
   VMCS, which is called VMCS region. VMCS regions are organized into
   six groups: Guest-State area, Host-State area, VM-execution control
   fields, VM Exit control fields, VM Entry control fields, and VM
   Exit information fields. Each of them contains one aspect of VMX
   information. For example, both Guest-state area and Host-state area
   contain the fields that corresponding to different components of
   processor state.  When VM Exits happen, processor states of guest
   are saved to the Guest-state area and processor states are loaded
   from the Host-state area to restore host context. As shown in
   \figurename \ref{fig:vmx}, VMX also provides several instructions
   to manage VMCS regions.

   The remaining parts of this paper frequently use VMCS to refer to a
   VMCS region associated to one logical processor.

   EPT (Extended Page Table)\cite{intel-vt} is a hardware extension
   for optimizing performance of memory virtualization. When EPT is
   active, separate page tables are provided to translate
   guest-physical addresses to the host-physical addresses. Meanwhile
   the traditional page tables finish the translation from guest-liner
   address to guest-physical address.

   EPT takes over the technique of shadow page table, avoids the
   expensive VM Exits and complex handling procedures of guest page
   faults, and therefore brings programming flexibility and
   performance improvement. Besides, EPT avoids memory usage of shadow
   page table which needs a whole copy of guest page tables.

** KVM
   KVM (Kernel-based Virtual Machine)\cite{kvm} is a virtualization
   solution integrated in Linux kernel, which consists of a loadable
   kernel module that provides the core virtualization infrastructure
   and a processor specific module. As a kernel module in Linux, KVM
   leverages existing Linux features and provides an integrated VMM
   approach. Virtual CPUs (vCPUs) of KVM guests are normal threads in
   the host OS, while memories of KVM guests are mapped into the
   memory space of their corresponding threads. KVM is a relatively
   new but mature virtualization solution for Linux on x86
   architecture. Studies show the KVM has comparable performance to
   Xen\cite{quant-comp}.

* Architecture
  Using QEMU\cite{qemu}, KVM is able to run nested virtualization with
  low performance compare to conventional virtualization.  Guest's
  code can be accelerated on the physical processor by virtualization
  extensions. In the nested environment however, there is only one VMM
  can run on the real hardware and utilize hardware extensions. The
  nested VMM only has a hardware layer provided by the underlying VMM,
  which has no hardware extension.

#+CAPTION: Three-Level Nested Virtualization Architecture
#+LABEL: fig:threelv
[[./nested3.eps]]

  We designed NestCloud, a three-level architecture for nested
  virtualization.  NestCloud provides the ability to use the hardware
  extensions for the nested VMM. \figurename \ref{fig:threelv}
  represents the architecture of NestCloud. \figurename
  \ref{fig:threelv} can be separated into two parts. Part A is the
  traditional architecture, which includes a normal guest and a VMCS
  associated with the vCPU (virtual CPU) where guest OS OS1 runs
  on. Part B is the architecture of NestCloud, which consists of three
  levels. In level 0 runs L0 VMM, which is a modified VMM running on
  the real hardware. Components in level 1 can either be a guest or a
  VMM. Component in level 1 is called L1 VMM when it is a VMM, and L0
  VMM is transparent to it. Hardware layer of L1 VMM is provided by L0
  VMM. Like a typical VMM, L1 VMM can create its own guest. Components
  on Level 2 are nested guests, which are called L2 Guest in this
  paper.

  In NestCloud, no modification on L1 VMM or L2 Guest OS is
  needed. Optimizations provided in the following sections may need
  slight modification on L1 VMM, and we will discuss it later.

  Focusing on VMX extension, only L0 VMM runs in VMX root operation
  mode. L1 VMM and L2 Guest run in VMX non-root operation
  mode. NestCloud provide a nested VMX interface to L1 VMM in order to
  accelerate L2 Guest using VMX extension. The following subsections
  explains the nested interface.

#+CAPTION: Non-Nested Virtualization CPU Execution Flow
#+LABEL: fig:non-nest-flow
[[./nested4.eps]]

#+CAPTION: Nested Virtualization CPU Execution Flow
#+LABEL: fig:nest-flow
[[./nested5.eps]]

** Nested VMX Interface
   As we described in Section \ref{sec-2}, VMCS, which controls the
   transition of two operation modes, is the most important component
   in VMX. In conventional virtualization, one VMCS is associated with
   one logical processor. In nested virtualization, the L1 VMM not
   only has its own logical processor (intrinsic vCPU), but also has
   L2 Guest's logical processor inside (shadow vCPU). When L2 Guest is
   running, the VMCS of its logical processor is supposed to be
   associated with the physical processor, thus the support of VMCS
   needs to be extended.

   NestCloud proposes three concepts of VMCS: the intrinsic VMCS
   (iVMCS), the shadow VMCS (sVMCS) and the physical VMCS (pVMCS). The
   first two are correspond to the L1 VMM's VMCS and the L2 Guest's
   VMCS. The last one is the VMCS used by the physical processor. They
   have the relationship as following:

#+BEGIN_LaTeX
   \begin{numcases}{pVMCS=}
   iVMCS & when running in L1 Guest\\
   sVMCS & when running in L2 Guest
   \end{numcases}
#+END_LaTeX

   For VMX instructions, NestCloud uses the traditional
   trap-and-emulate method. VMX instructions issued by L1 VMM will
   cause VM Exit and be trapped into L0 VMM. Using instruction
   parameters got from VM Exit reasons, L0 VMM handles the requests
   and operations on the real VMX extension. In this way, L1 VMM can
   use VMX extension to improve the performance of L2 Guests.

** Nested CPU Execution Flow
   In a non-nested guest, the execution flow with VMX is shown in
   \figurename \ref{fig:non-nest-flow}. At time A, the VMM issues a VM
   Entry instruction to wake up the guest, and the system turns into
   non-root operation mode. During T2, guest's instructions are
   executed on the physical processor directly. At time B, VM Exit
   happens, and the processor execution turns back to the VMM to
   handle the VM Exit event.

   \figurename \ref{fig:nest-flow} is the CPU execution flow in
   NestCloud, which involves the three levels' interaction. At time A,
   L0 VMM issues a VM Entry to turn on L1 VMM. L1 VMM issues the
   virtual VM Entry at time B, which causes a VM Exit and the switch
   of VMCS from VMCS2(iVMCS) to VMCS21(sVMCS). At time C, L0 VMM
   issues the real VM Entry which calls up L2 Guest. So far, the L2
   Guest can get a running opportunity during T4. The L2 Guest keeps
   running on the physical processor until a virtual VM Exit happens
   at time D.

** Handling VM Exits
   The procedure of handling VM Exits from L2 Guest differs in
   NestCloud. Unlike non-nested situation, where VM Exits are all
   handled by the VMM. In NestCloud, L0 VMM needs to decide the
   handler of VM Exits. If a VM Exit is due to L0 VMM, shadow page
   faults and external IRQs for example, L0 VMM handlers will handle
   it.

   If L1 VMM is responsible for the VM Exit, L1 VMM should be turned
   on to handle it. In this situation, pVMCS needs to be switched to
   iVMCS, and a virtual VM Exit needs to be injected into L1 VMM. The
   virtual VM Exit is constructed according to EXIT\_REASON in
   vVMCS. If the switch is due to virtual IRQs, a new EXIT\_REASON is
   generated.

   If the VM Exit is due to L2 Guest, L0 VMM will inject a virtual VM
   Exit to L1 VMM, and L1 VMM will read the VM Exit reason and inject
   it to L2 Guest. Events such as L2 page faults are handled this way.

* Implementation
  In this section, we describes the implementation details of NestCloud.

#+CAPTION: Nested VMCS Design
#+LABEL: fig:design
[[./nested6.eps]]

** Nested VMCS Implementation
   In nested VMCS implementation, the iVMCS for L1 VMM is in the L0
   VMM's memory space. The sVMCS is constructed by L0 VMM according to
   VMCS for L2 Guest in the L1 VMM's memory space, which is also
   called vVMCS. In order to simplify the procedure of accessing
   vVMCS, a copy of vVMCS is kept in L0 VMM's memory and synchronized
   with L1 VMM. \figurename \ref{fig:design} represents their
   relationships.

** Trap-and-emulation of VMX Instructions
   When L1 VMM issues a VMX instruction, it generates a VM Exit which
   is trapped by L0 VMM. A handler in L0 VMM will handle the VMX
   instructions on behalf of the L1 VMM.  These handlers take
   advantages of the real VMX extension which makes the
   performance of L2 Guest close to L1 Guest.

   Five VMCS maintenance instructions and five VMX management
   instructions are provided by VMX extension\cite{intel-vt}, and all
   of them has a corresponding handler in L0 VMM. Here we describe
   implementation details of some important instructions handlers.

*** Virtual VMPTRLD/VMPTRST Handling
    VMPTRLD \cite{intel-vt} loads the current VMCS region pointer from
    memory. The handler of VMPTRLD fetches the address of the new VMCS
    region by decoding the VM Exit reason, and synchronizes the L0
    VMM's copy of vVMCS. For later reference, the address of the new
    VMCS region is also saved in L0 VMM. VMPTRST stores the current
    VMCS pointer into memory, and the handler is similar. The vVMCS in
    L1 VMM is synchronized with the copy in L0 VMM, and the saved
    address is returned.

*** Virtual VMCLEAR Handling
    VMCLEAR ensures all fields of VMCS are copied to VMCS
    region\cite{intel-vt}. The handler of this instruction just
    synchronizes the L0 VMM's cached copy with the vVMCS in L1 VMM's
    memory.

*** Virtual VMREAD/VMWRITE Handling
    VMREAD reads a specified VMCS field\cite{intel-vt}. The handler
    works as follows: (1) Decoding VMREAD information from the exit
    information of VM Exit. (2) Reading the specified field from the
    L0 VMM's vVMCS copy. (3) Saving the value to the specified
    register in the exit information. The handler of VMWRITE works
    similar. It does the writing on vVMCS copy instead of reading.

*** Virtual VMLAUNCH/VMRESUME Handling
    These two instructions launch or resume a guest managed by current
    VMCS and then transfer control to the guest\cite{intel-vt}. They
    are handled in the same way in nested virtualization environment.
    In \figurename \ref{fig:non-nest-flow}, "VMENTRY" and "Virtual
    VMENTRY" are examples of these two instructions. VMPTRST, VMPTRLD
    and VMCLEAR are preparations of these two instructions. The pVMCS
    differs before and after the VMRESUME. It points to iVMCS when L1
    VMM is running, and points to sVMCS when L2 Guest is running. When
    L0 VMM handles VMRESUME, the pVMCS should be switched from iVMCS
    to sVMCS. After pVMCS switching, L0 VMM can enter L2 Guest by a
    real VMRESUME instruction.

* Optimizations
  Section \ref{sec-4} introduces the implementation of NestCloud. In
  this section we describe the optimizations on NestCloud. The goal of
  optimizations is to eliminate the performance gap between L2 Guest
  and L1 Guest. We provide 3 optimizations including Guest Page Fault
  Bypassing, Virtual EPT and PV VMCS. The idea of these optimizations
  is to reduce the transitions between L0, L1 and L2, which are
  considered as one of the root causes of the overhead.

** Guest Page Fault Bypassing
   Page faults can occur for a variety of reasons. In some cases, page
   faults alert the VMM to an inconsistency between the page table and
   its shadow copy\cite{shadow}. In other cases, the hierarchies are
   already consistent and the page fault should be handled by the
   guest OS. The formal cases are called shadow page faults and can
   only be handled by the VMM, while the latter cases do not need
   interceptions of VMM at all.

   The optimization of guest page fault bypassing makes the L2 Guest
   handle its own page faults without causing a VM Exit to save
   transition time. It is implemented by a feature of VMX.  VMX
   provides 2 registers in VMCS: PFEC\_MASK and PFEC\_MATCH. When the
   page fault error code (PFEC) matches these 2 registers (PFEC &
   PFEC\_MASK = PFEC\_MATCH), the page fault will be delivered through
   guest's IDT without causing a VM Exit\cite{intel-vt}. In this
   optimization, PFEC\_MASK and PFEC\_MATCH are set to 1, so that page
   faults caused by non-present pages do not cause VM Exit at all. The
   key information to separate 2 page fault cases is that the reason
   of shadow page fault cannot be non-presented pages. In such a way,
   only page faults of L2 Guest are bypassed.

   Not all page faults of L2 Guest are caused by non-presented
   pages. This optimization does not work for the page faults caused
   by illegal access or other reasons. To judge the effectiveness of
   this optimization, we collect the count of page faults during a
   kernel building. KVMTrace\cite{kvm} is a module in Linux
   kernel which can record the KVM event timestamps and event
   parameters. It is used to count the page faults of VM Exit from L2
   Guest.

   Page faults coming from L2 Guest are separated into 3 categories:
   (1) L0 shadow page fault, which is solved by L0 directly; (2) L1
   shadow page fault, which is injected into and handled by L1 VMM;
   (3) L2 page fault, which is injected into L2 guest through L1
   VMM. The expected effect of this optimization is reducing the count
   of L2 page faults we caught.
   
#+CAPTION: Guest Page Fault Bypassing in Kernel Building
#+LABEL: fig:bypass-pf
[[./nested7.eps]]

   \figurename \ref{fig:bypass-pf} shows a 60 seconds sample of page
   fault count. In the meantime, we get a 5% performance gain during
   kernel building. The count of VM Exits caused by L2 page faults is
   reduced by 35% after the guest page fault bypassing. In the
   meanwhile, the L0 shadow page fault is increased by 6.2% due to the
   performance gain (L2 Guest did more during 60 seconds sample).
   Because only 13.13% of page faults are L2 page faults, the
   performance gain is not as good as we expected.

** Virtual EPT Support
   EPT can largely improve guest's performance. In this optimization,
   a concept of virtual EPT is proposed. Virtual EPT support is used
   in L1 VMM and works for L2 Guest's page table. Consequently, the
   EPT support provided by hardware is called host EPT.

   Host EPT has already been supported by KVM as we described in
   Section \ref{sec-2}. It also creates a great performance gain on
   nested virtualization. But currently, EPT has not been supported in
   L1 VMM. Address translation of L2 Guest has to use the shadow page
   table mechanism and causes a lot of VM Exits.

#+CAPTION: Virtual EPT Support
#+LABEL: fig:vept
[[./nested8.eps]]

   We present a full EPT interface to L1 VMM by trapping all the EPT
   events from L1 VMM, and forward them directly to the real
   hardware. Meanwhile, the hardware EPT events are injected into L1
   VMM by L0 VMM, such as EXIT\_REASON\_EPT\_VIOLATION and
   EXIT\_REASON\_EPT\_MISCONFIG. With virtual EPT, VM Exit by shadow
   page table will be significantly reduced and the performance can
   get a boost.  Notice that virtual EPT is supported only when the
   host EPT is enabled, because the virtual EPT is implemented by
   forwarding events to the host EPT. \figurename \ref{fig:vept} shows
   how the host EPT and virtual EPT work.

#+CAPTION: L1 VMM Events Breakdown
#+LABEL: tbl:vmevents
   |-----------+------------|
   | Event     | Percentage |
   |-----------+------------|
   | VMREAD    |        67% |
   | VMWRITE   |        19% |
   | Exception |         7% |
   | VMRESUME  |         6% |
   | Others    |         1% |
   |-----------+------------|

** PV VMCS
   In order to uncover the performance bottleneck of L1 VMM, we
   collected statistic information on the VMX events during kernel
   building. Table \ref{tbl:vmevents} is the breakdown of all events
   in L1 Guest VM Exit reasons. 86\% of VM Exits are due to VMREAD and
   VMWRITE.  Before optimization, every time when L1 VMM accesses a
   vVMCS field, VMREAD or VMWRITE causes a transition from L1 VMM to
   L0 VMM, and L0 VMM will access the field in vVMCS copy. Actually,
   L1 VMM has its own copy of vVMCS, thus it has full knowledge to
   perform VMREAD and VMWRITE by itself.

#+CAPTION: Before PV VMCS Optimization
#+LABEL: fig:before-opt
[[./nested10a.eps]]

#+CAPTION: After PV VMCS Optimization
#+LABEL: fig:after-opt
[[./nested10b.eps]]

   In order to enable vVMCS access in L1 VMM, we need to expose vVMCS
   layout and accessing method in L1 VMM. Besides, L0 VMM should be
   slightly modified too. As we mentioned in Section \ref{sec-3}, L0
   VMM holds a vVMCS copy, which is synchronized with vVMCS in L1's
   memory. This copy should be updated explicitly in this
   optimization. \figurename \ref{fig:before-opt} and
   \ref{fig:after-opt} shows the PV VMCS optimization of VMREAD.

   The effect of PV VMCS varies according to different
   applications. The PV VMCS needs modifications on the L1 VMM, which
   is not applicable in some situations such as commercial
   virtualization solutions.

* Evaluation
  We have implemented NestCloud and the optimizations on
  KVM-84\cite{kvm}. In this section, we evaluate the performance
  of NestCloud. We try to prove that: (1) NestCloud is better than the
  nested solution of QEMU on KVM (2) With optimizations, the
  performance of NestCloud is close to that of L1 Guest on CPU and
  memory.

  Most evaluations have 7 situations: L1 (L1 Guest performance), QEMU
  (nested virtualization using QEMU emulation with host EPT), Basic
  (implementation of NestCloud with no optimization), Bypass (using
  both L1 VMM and L2 Guest page fault bypassing), PV VMCS (BASIC with
  PV VMCS), Host EPT (BASIC with host EPT), Host/Virtual EPT (BASIC
  with host and virtual EPT), Host/Virtual EPT + PV VMCS (BASIC with
  host EPT, virtual EPT, and PV VMCS). Our goal is to make the
  performance of L2 Guest close to a normal guest (performance of L1
  Guest with host EPT), thus some results are normalized to L1.

** Environment and benchmarks
   We performed all experiments on a server with a VT-enabled Intel
   core i7-920 and 6 GB memory. The host/guest OS used in our tests is
   Ubuntu 9.04. The L0 VMM's kernel is KVM-84\cite{kvm} with
   NestCloud; the L1 Guest's kernel is KVM-84 with no modification;
   and the L2 Guest uses original kernel of Ubuntu 9.04. To make the
   L2 Guest time accurate, KVM PV-TIMER module (CONFIG\_KVM\_CLOCK=y)
   is enabled in the L2 Guest kernel.

   VMX extension is used for CPU virtualization, which is the focus of
   our tests. SPEC CPU 2006\cite{speccpu,speccpu-io} is an
   industry-standardized, CPU-intensive benchmark suite. It contains
   two test packages: CINT tests and CFP tests. Benchmarks in SPEC CPU
   2006 are derived from real world applications. They spend at least
   95% of its execution time in user space\cite{speccpu-io}.
   SysBench-CPU\cite{sysbench} uses calculation of prime numbers up to
   a specified value, and the result is valued in running time.

   In addition, we use SysBench-Memory\cite{sysbench} to measure the
   memory performance. To get I/O performance, SysBench
   OLTP\cite{sysbench} is used. OLTP stands for On-Line Transaction
   Processing. SysBench OLTP keeps generating transactions for MySQL
   when it is running.

#+BEGIN_LaTeX
\begin{figure*}[htb]
\includegraphics{./nested11.eps}
\caption{SPEC CPU 2006 CINT Results}
\label{fig:spec_cint}
\end{figure*}
#+END_LaTeX

#+BEGIN_LaTeX
\begin{figure*}[htb]
\includegraphics{./nested12.eps}
\caption{SPEC CPU 2006 CFP Results}
\label{fig:spec_cfp}
\end{figure*}
#+END_LaTeX

** CPU Performance
#+CAPTION: SysBench-CPU Results
#+LABEL: tbl:sysbench-cpu
   |---------------------------------+------------|
   |                                 | Results(s) |
   |---------------------------------+------------|
   | L1                              |    36.0535 |
   | Basic                           |    38.2076 |
   | Bypass                          |    38.7977 |
   | Host EPT                        |    40.7520 |
   | Host EPT + Virtual EPT          |    38.4142 |
   | PV VMCS                         |    37.8735 |
   | PV VMCS, Host EPT + Virtual EPT |    37.9351 |
   | QEMU                            |   785.7888 |
   |---------------------------------+------------|

   The results of SysBench-CPU is presented in Table
   \ref{tbl:sysbench-cpu}. Differences between Basic situation and
   situations with optimizations are quite small, and they are about
   21 times faster than QEMU. In the situation of Host/virtual EPT and
   PV VMCS, L2 Guest introduces 5.22% overhead compare to L1 Guest.

   The VMX interface of NestCloud enables the L2 Guest's instruction
   to execute on the physical CPU directly. In a CPU-intensive
   benchmark like SysBench-CPU, the overhead of an additional level is
   quite small.

   SPEC CPU 2006 on QEMU nested environment has very low performance,
   and some benchmarks fail to get a result. Here we only provide
   bzip2 and gcc results in Table \ref{tbl:cpu2006}, which shows that
   the QEMU nested virtualization can only get about 5% of a L1
   Guest's performance.

#+CAPTION: QEMU Nested SPEC CPU 2006 Results
#+LABEL: tbl:cpu2006
   |-------+-----+-------|
   |       |  L1 |  QEMU |
   |-------+-----+-------|
   | bzip2 | 756 | 11872 |
   | gcc   | 420 |  8109 |
   |-------+-----+-------|

   \figurename \ref{fig:spec_cint} shows 12 results of CINT
   benchmarks, and \figurename \ref{fig:spec_cfp} shows the results of
   CFP benchmarks. These results are normalized to L1 Guest's
   results. Compare to SysBench-CPU, SPEC CPU 2006 is a mixed
   benchmark, which consists of CPU workload, memory workload and a
   little bit of I/O workload. The effects of optimizations varies
   between different tests.

*** Effect of virtual EPT
    Virtual EPT works extremely well in some of the benchmarks,
    including gcc in CINT, soplex and tonto in CFP. After an
    investigation on these benchmarks, we figure out that these
    benchmarks perform many memory allocations and
    freeings\cite{speccpu-mem-footprint}. These activities lead to
    page table changes, and therefore provide bad results with shadow
    page table. In the following subsection, we will discuss
    performance of shadow page table in detail.

    Also, virtual EPT does not work in some cases, including sjeng,
    xalancbmk in CINT and bwaves, zeusmp and lbm in CFP. The
    performance of Intel EPT has lower performance under: (1) little
    MMU activity (2) high TLB miss rate\cite{perf-ept}. And, all these
    benchmarks have relatively higher TLB miss
    rate\cite{speccpu-perf-counter}, together with few memory
    allocation/freeing activities\cite{speccpu-mem-footprint}.

*** Effect of PV VMCS
    Actually, PV VMCS is a trade-off that works only when the
    frequency of VMREAD and VMWRITE is high enough. In a rare case,
    the synchronization cost of vVMCS is larger than the performance
    gain, and this optimization will get worse result. The test of
    libquantum in CINT is an example. PV VMCS works for it, but does
    not work when virtual EPT is also applied. The reason is that
    virtual EPT will significantly reduce the VMREAD/VMWRITE caused by
    page faults, and PV VMCS will not work as good as before. Similar
    results can be found in the test of PF-Bench following.

   In conclusion, L2 Guest with optimizations can achieve 88.08% of L1
   Guest in CINT benchmarks and 85.68% of L2 Guest in CFP benchmarks,
   which means 13.53% and 16.71% overhead.

** Memory Performance
#+CAPTION: SysBench-Memory Results
#+LABEL: tbl:sysbench-mem
   |---------------------------------+------------|
   |                                 | Results(s) |
   |---------------------------------+------------|
   | L1                              |    54.1131 |
   | Basic                           |    57.6744 |
   | Bypass                          |    57.3680 |
   | Host EPT                        |    57.3903 |
   | Host EPT + Virtual EPT          |    57.3920 |
   | PV VMCS                         |    56.6564 |
   | PV VMCS, Host EPT + Virtual EPT |    56.5042 |
   | QEMU                            |   647.9132 |
   |---------------------------------+------------|

   Table \ref{tbl:sysbench-mem} shows the result of
   SysBench-Memory. Similar to SysBench-CPU results, Basic situation
   and optimized situation vary slightly. Also, they are about 11
   times faster than QEMU because of the VMX interface. The best
   result of SysBench-Memory presents 5.69% overhead compare to L1
   Guest.

#+CAPTION: PF-Bench Results
#+LABEL: tbl:pfbench
   |---------------------------------+------------|
   |                                 | Results(s) |
   |---------------------------------+------------|
   | L0 Performance                  |       1.37 |
   | L1                              |      23.85 |
   | Basic                           |     501.01 |
   | Bypass                          |     470.25 |
   | Host EPT                        |     358.98 |
   | Host EPT + Virtual EPT          |       2.39 |
   | PV VMCS                         |      71.01 |
   | PV VMCS, Host EPT + Virtual EPT |        5.6 |
   | QEMU                            |      35.90 |
   |---------------------------------+------------|

   In order to measure our optimization effort on page faults, we
   design a micro-benchmark called PF-Bench, which keeps generating
   page faults when it is running. Page faults in L2 Guest without any
   optimization are heavy. Each of them triggers several VM Exits and
   VM Entries, and lets the CPU go back-and-forth between L0 VMM and
   L1 VMM. When L2 Guest is handling page faults, it modifies the page
   table, and triggers a L1 shadow page fault. When the memory pages
   of L2 Guest page faults are also absent from L1 VMM's page table,
   they trigger another page faults of L1 VMM. Furthermore, L1 VMM can
   also trigger L0 shadow page faults when it is modifying its page
   table. Every page fault from L2 Guest triggers a page fault chain,
   which cost much CPU time.

   The results are given as running time in Table
   \ref{tbl:pfbench}. Bypass works for page faults of L2 Guest. It
   eliminates the back-and-forth of L2 Guest page fault, and has a
   6.54% performance gain. Host EPT works for L0 shadow page faults,
   and it has an acceleration of 39.56%. PV VMCS largely reduces the
   cost of VM Entry and VM Exit between L1 VMM and L2 Guest, and has a
   speedup of 605.55%. The best optimization is virtual EPT, it is
   150+ times faster than Basic. The result of QEMU is better than
   Basic, and even better than several optimized situations such as
   Bypass and Host EPT. This is because QEMU does not use shadow page
   table, and avoids the heavy work of back-and-forth between levels.

** I/O Performance
#+CAPTION: SysBench-OLTP Results
#+LABEL: tbl:sysbench-oltp
   |---------------------------------+--------------|
   |                                 | Results(t/s) |
   |---------------------------------+--------------|
   | L1                              |          535 |
   | Basic                           |        13.92 |
   | Bypass                          |        16.34 |
   | Host EPT                        |        16.19 |
   | Host EPT + Virtual EPT          |        44.38 |
   | PV VMCS                         |        19.12 |
   | PV VMCS, Host EPT + Virtual EPT |        48.96 |
   | QEMU                            |        13.23 |
   |---------------------------------+--------------|

   Table \ref{tbl:sysbench-oltp} is the test results of SysBench OLTP
   benchmark. The performance of L2 Guest is only 10% of the L1
   Guest's. The low performance of I/O in L2 is understandable, since
   all the I/O operations needs back-and-forth between 3 levels just
   like the situation of page fault. However, the best optimization
   result is 3.7 times better than the QEMU nested.

   In this paper, we do not explicitly optimize the I/O
   performance. The OLTP test uses emulated I/O, which depends on IRQ
   injection and foreign memory accessing. They are heavy in L1 VMM,
   because they all need interception of L0 VMM. Optimizations on them
   are listed as future work.

* Related Work
  Nested virtualization (A.K.A recursive virtualization) has a history
  of more than 30 years. In 1976, the Kernelized VM/370 was able to
  run a VMM recursively in a virtual machine but suffered from
  performance\cite{sysbench}. A study by Hugh et al.\cite{recur-vm}
  proposes a computer system with recursive virtual machine
  architecture, whose central idea is the ability of any process to
  define a new virtual memory within its own virtual memory. Based on
  this idea, Bryan et al.\cite{micro-vm} use the micro-kernel to
  propose a novel approach to develop a software-based virtualizable
  architecture called Fluke. Fluke allows recursive virtual machine,
  and can easily deploy arbitrary level of nested virtual machines.

  Blue Pill\cite{bluepill} is targeted for security in Windows. It is
  a thin VMM to control the OS and is responsible for controlling
  "interesting" events inside the guest OS. Nested virtualization is
  one of the features it supports, and is implemented on AMD SVM. IBM
  z/VM\cite{zvm} VMM also supports running a nested z/VM OS, but is
  intended only for testing purposes, and do not care much about the
  performance\cite{ibm-vm-faculty}.

  The turtles project\cite{turtles} is a recent solution for nested
  virtualization. It has a different idea from us. It multiplexes
  multiple levels of virtualization into one level on CPU
  virtualization. On memory virtualization, it uses an idea of
  multi-dimensional page table. Compare to their evaluation, NestCloud
  get a similar performance overhead.

  To make virtualization much easier and faster, lots of studies have
  been performed in both software
  fields\cite{opt-net,bridge-gap-sw-hw,virtio-multicore,perf-10g,imp-vt-perf-scal}
  and hardware fields\cite{intel-vt,amd-v,hp-net-sriov}, but they do
  not address efficiency of nested virtualization.

* Conclusions and Future Work
  Nested virtualization can be used in several usage models such as
  debugging and live migration. In this paper we present the design,
  implementation and evaluation of NestCloud, a three-level nested
  virtualization architecture for practical high performance nested
  virtualization. We have minimized the overhead caused by the
  additional level by three optimizations. The evaluation demonstrates
  that the implementation of NestCloud introduces 5.22% overhead on
  CPU and 5.69% overhead on memory, and is close to a conventional
  one.

  The I/O performance of NestCloud is relatively low compared to a
  conventional guest, and optimizing it is the most relevant future
  work. I/O virtualization bypassing which bypasses an I/O device in
  L1 VMM to L0 VMM is a potential optimization. Direct access to I/O
  devices for L2 Guests can also be a solution. In addition, the
  support of SMP is another future work, which needs to deal with
  problems such as vCPU migration. The live migration of L2 Guest to
  other L1 VMM and L0 VMM on the same physical machine is also an
  interesting future work.

#+LATEX: \section*{Acknowledgments}
  This work is supported by the National Natural Science Foundation of
  China (Grant No. 61170050).

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{nested}

* Comments                                                        :noexport:
** Review 2.1
  > *** Summary of the paper: Summary of the paper

  The paper implements a mechanism for nested virtualization in KVM.

  > *** Paper Evaluation: What are the major issues addressed in the paper? Do you consider them important?

  Comment on the degree of novelty, creativity, impact, and technical
  depth in the paper. What are the major reasons to accept the paper?
  What are the most important reasons NOT to accept the paper?

  The novelty of this work is low, as the methods for nested
  virtualization are already known in the virtualization
  community. The optimizations are useful and interesting hacks, but
  are not major research contributions.

  > *** Relevance to the conference: <b> The focus of HiPC 2010 is on
        current research in all areas of high performance computing
        including design and analysis of parallel and distributed
        systems, embedded systems, and their applications in
        scientific, engineering, and commercial areas. Please rank the
        relevance of the reviewed work to the theme of the
        conference. Note that this rating is independent of the
        overall rating.</b>

  Good (4)

  > *** Technical soundness: How would you score the technical merits
  of the paper?

  Good (4)

  > *** Originality: Originality level of the contribution?

  Weak (2)

  > *** Quality of the presentation: Readability, English, graphics, etc.

  Weak (2)

  > *** Level of confidence: What is your level of confidence/expertise for this review?

  Strong (5)

  > *** Recommendation: Your overall rating

  Weak Reject (2)

  > *** Detailed Comments: Please provide detailed comments that will
        be helpful to the TPC for assessing the paper. Also provide
        feedback to the authors.

  Nested virtualization is already been implemented in Xen and I
  believe KVM developers are working on it as well. Google search for
  "kvm nested virtualization" brings up various mailing list threads.

  The virtualization of VMCS is a straight-forward way of implementing
  nested virtualization, and involves more engineering than research.

  The paper looks at nested virtualization as just two-level
  virtualization, rather recursive virtualization to infinity. There
  is no discussion on how you would run a three-level nested
  virtualization. Optimizations like guest page-fault bypassing will
  need to be re-worked in this case. Perhaps, three-level nested
  virtualization is not useful, but discussion on how to handle it is
  important.

  Overall, this is good engineering work, and It would be nice to see
  distillation of the core systems ideas for nested virtualization.

** Review 2.2
  > *** Summary of the paper: Summary of the paper

  This paper describes the design and implementation of an nested
  Virtualization system based on the Intel VMX intstruction set. This
  allows a virtual machine created with KVM to use a second
  virtualization layer, without resorting to purely software solutions
  such as QEMU. There are several reasons for wanting nested
  virtualization, including debugging of virtualization systems.

  > *** Paper Evaluation: What are the major issues addressed in the
        paper? Do you consider them important?

  Comment on the degree of novelty, creativity, impact, and technical
  depth in the paper. What are the major reasons to accept the paper?
  What are the most important reasons NOT to accept the paper?

  > *** Relevance to the conference: <b> The focus of HiPC 2010 is on
        current research in all areas of high performance computing
        including design and analysis of parallel and distributed
        systems, embedded systems, and their applications in
        scientific, engineering, and commercial areas. Please rank the
        relevance of the reviewed work to the theme of the
        conference. Note that this rating is independent of the
        overall rating.</b>

  Weak (2)

  > *** Technical soundness: How would you score the technical merits
        of the paper?

  Normal (3)

  > *** Originality: Originality level of the contribution?

  Weak (2)

  > *** Quality of the presentation: Readability, English, graphics,
        etc.

  Poor (1)

  > *** Level of confidence: What is your level of
        confidence/expertise for this review?

  Normal (3)

  > *** Recommendation: Your overall rating

  Weak Reject (2)

  > *** Detailed Comments: Please provide detailed comments that will
        be helpful to the TPC for assessing the paper. Also provide
        feedback to the authors.

  Although the topic of nested virtualization is an interesting one in
  itself, the low level of the presentation, including language and
  structure of the paper, make it difficult to read this paper.

  The paper seems to describe mostly an engineering effort to get
  nested virtualization to work, not scientific research. A clear
  listing of all contributions would significantly improve this paper.

  The KVM people have been working on nested virtualization too, both
  for ATI
  (http://avikivity.blogspot.com/2008/09/nested-svm-virtualization-for-kvm.html)
  and Intel
  (http://avikivity.blogspot.com/2009/09/nested-vmx-support-coming-to-kvm.html)
  architectures, further strengthening the impression this is a
  software engineering problem.

  Minor comment: Some of the acronyms used are never introduced.

** Review 2.3
  > *** Summary of the paper: Summary of the paper

  The authors propose a new three-level nested virtualization
  architecture in Linux kernel, minimizing the overhead caused by the
  additional virtualization level with optimizations.

  > *** Paper Evaluation: What are the major issues addressed in the
        paper? Do you consider them important?

  Comment on the degree of novelty, creativity, impact, and technical
  depth in the paper. What are the major reasons to accept the paper?
  What are the most important reasons NOT to accept the paper?

  The major issue of the paper is the topic of vitualization and the
  prosed optizations.

  The discussion of the research is not clear, there are a lot of
  informations but there is a lack of objectivity in the presentations
  of the results.

  > *** Relevance to the conference: <b> The focus of HiPC 2010 is on
        current research in all areas of high performance computing
        including design and analysis of parallel and distributed
        systems, embedded systems, and their applications in
        scientific, engineering, and commercial areas. Please rank the
        relevance of the reviewed work to the theme of the
        conference. Note that this rating is independent of the
        overall rating.</b>

  Good (4)

  > *** Technical soundness: How would you score the technical merits
        of the paper?

  Normal (3)

  > *** Originality: Originality level of the contribution?

  Good (4)

  > *** Quality of the presentation: Readability, English, graphics, etc.

  Normal (3)

  > *** Level of confidence: What is your level of confidence/expertise for this review?

  Good (4)

  > *** Recommendation: Your overall rating

  Weak Reject (2)

  > *** Detailed Comments: Please provide detailed comments that will
        be helpful to the TPC for assessing the paper. Also provide
        feedback to the authors.

  The authors discuss an old but important subject, virtualization,
  that now is returning with the multi-core architectures.

  In the section V it is presented the evaluation of the proposed
  nested virtualization with suggested optimizations for CPU tests,
  Memory tests and I/O tests . For the I/O mesurements the authors
  informs that "due the limit of time" they "haven't optimized" So the
  proposed optimizations where not used on this test.

  As we mentioned above, the paper discusses an important topic but
  there is a lack on the discussion of the research and on the
  presentation of the paper that the authors have to correct.

** REVIEW 1.1
   OVERALL RATING: 2 (accept (I would be happy accepting this paper, but
   I wouldn't fight for it))
   REVIEWER'S CONFIDENCE: 2 (medium)
   Originality: 4 (good (top 25%, but not top 10%))
   Technical Merit: 4 (good (top 25%, but not top 10%))
   Readability: 3 (fair (top 50%, but not top 25%))
   Relevance to Conference: 5 (excellent (top 10%))
   Candidate for Best Full Paper?: 2 (no)
   Candidate for Best Short Paper?: 2 (no)
   Candidate for Best Student  Full Paper?: 2 (no)
   Candidate for Best Student Short Paper?: 2 (no)

   - not compliant to conference style
   - english could be improved, e.g. sometimes 'a' missing
   - the third level structuring of 3.2 could be removed
   - check text in Figure 7
   - Especially Chapter 4 got many illustrations/tables: (a) could be
   reduced in size and (b) described a bit more (c) many partly
   removed/combined
   - good evaluations
   - remove thanks to reviewers rather mentioning your funding organizations

** REVIEW 1.2
   OVERALL RATING: -1 (weak reject (This paper is too weak for this conference))
   REVIEWER'S CONFIDENCE: 2 (medium)
   Originality: 3 (fair (top 50%, but not top 25%))
   Technical Merit: 4 (good (top 25%, but not top 10%))
   Readability: 2 (poor (bottom 50%, but not bottom 10%))
   Relevance to Conference: 2 (poor (bottom 50%, but not bottom 10%))
   Candidate for Best Full Paper?: 2 (no)
   Candidate for Best Short Paper?: 2 (no)
   Candidate for Best Student  Full Paper?: 2 (no)
   Candidate for Best Student Short Paper?: 2 (no)

   This paper targets the problem of nested virtualization. The
   authors have implemented 3 types of optimizations and have
   conducted experiments using standard benchmark. Some of the results
   are convincing regarding the fact that their optimizations can
   improve the performance of nested VMs. I feel that there are three
   problems with this paper:

   1) The presentation could be largely improved, as described in my
   comments hereafter.

   2) The experimental results could be analyzed more in depth. It
   would be nice if the authors had some idea of why some benchmarks
   benefit so much more from their optimization than others. For
   instance, why are the results for gcc in Figure 10 so incredibly
   different from other results? Why are the FP results in Figure 11
   so different from the INT results in Figure 10? What is special
   about these 7 benchmarks that perform so well using the authors'
   optimizations? I understand that it's difficult to have a definite
   explanation for each results, but at least some attempt should be
   made. It seems that looking at VMM logs would yield at least some
   hints. The paper could have used 2 more pages to explore the
   results more in depth and still be within the page limit. Also,
   results in Table 2 are obtained with a page-fault benchmark, and
   the only given details are "written by ourselves." This is not
   enough and the reader needs to know what this benchmark does.
   Section 5.4 does not give all results for the SysBench-Memory
   results.  This is a bit jarring. For one of the memory benchmark we
   have Table 3, and for the other one the text just says "The result
   is 94.62%". We don't even know which optimizations are used (i.e.,
   which of the 5 versions).  Figures 10 and 11 show results for 6
   versions (the 5 + the original). Table 3 shows results for 5
   versions, including one that's not in the figures.  This
   discrepancy is not explained/justified.  Similarly, Tables 4 and 5
   shows results for bypass and EPT, but not for PV VMCS. Overall, all
   these discrepancy have a very distracting effect. So, to summarize,
   the results are not sufficiently explained and their presentations
   have inconsistencies.

   3) This is basically a hard-core Operating Systems paper, and in
   this sense is not completely on-topic for the HPDC conference,
   which is about high-performance and distributed
   computing. Obviously virtualization has become an enabling
   technology for HPC, but the paper doesn't make much link with HPC
   or with Distributed Computing.

   Regarding 1) above, there are many problems that could be
   fixed. The description of the Nested Virtualization Design
   (Section 3) should be much clearer. The whole system is complicated
   due to the different levels, so it is important that the
   description be crystal clear.  Clearly, the authors are not native
   English speakers. Unfortunately, the English needs to be extremely
   tight for the content of Section 3 to be palatable. Also, the
   authors should better explain some of the existing VMM system. For
   instance, it would be nice to have a sentence explaining what
   VMENTRY and VMEXIT is. More generally, the paper throughout
   references system features / instructions of existing VM systems,
   and these should be introduced better for readers who are not
   familiar with the inner workings of VMM systems and hardware
   support for them. So, overall, the most technical parts of the
   paper are difficult to read and understand, although the overall
   approach used by the authors is understandable. With 2 extra pages,
   the authors could have explain things better.  I provide other
   detailed comments below:

   - Section 3.1 talks about time T2 in relationship with Figure 3, but
   there is no T2 in Figure 3.
  
   - In Section 4, a hint for future work regarding I/O is given and
   says "the possible solution could be direct I/O for L2 Guest". This
   should be reworded and explained better, i.e., "giving direct
   access to I/O devices for L2 Guests".

   - In Section 4, it is said that the experimental results are
   obtained on a system that's described in Section 5.1. This is a
   very odd forward reference. Typical one describes the system, and
   then in a later section say that the system is the same as the one
   described previously.

   - In Section 4.1, the sentence "If guest page fault bypassing...."
   is much too long and must be broken up in at least 2 sentences.

   - A very distracting thing in the paper is that Tables are often
   first referenced out of order. For instance, Table 2 is discussed
   before Table 1. Table 6 is referenced before Tables 3, 4, and 5.
   This must be fixed.

   - The last paragraph of Section 4.2 is just very confusing and
   unclear. In fact, it is not clear what the message of Section 4.2
   is, and by the end of it the reader doesn't have a clear idea of
   what the conclusion is. Furthermore, the last paragraph talks about
   EPT, which is only described in Section 4.2. Clearly, the two
   optimizations are not independent, which makes them a bit difficult
   to describe, but the paper doesn't really do a good job and
   addressing this difficulty. The last sentence of the section is
   also not enough: "results are not as expected.". More explanation
   is needed.

   - Although section 4.2.1 is supposed to be about Host EPT, it talks
   a lot about Virtual EPT, which is supposed to be the topic of
   Section 4.2.2.

   - Perhaps I missed it, but I don't think the text
   references/explains Figure 9.

** REVIEW 1.3
   OVERALL RATING: 1 (weak accept (I would be OK with accepting this paper))
   REVIEWER'S CONFIDENCE: 3 (high)
   Originality: 3 (fair (top 50%, but not top 25%))
   Technical Merit: 4 (good (top 25%, but not top 10%))
   Readability: 2 (poor (bottom 50%, but not bottom 10%))
   Relevance to Conference: 5 (excellent (top 10%))
   Candidate for Best Full Paper?: 2 (no)
   Candidate for Best Short Paper?: 2 (no)
   Candidate for Best Student  Full Paper?: 2 (no)
   Candidate for Best Student Short Paper?: 2 (no)

   This paper describes the design and implementation of nested
   virtualization using Linux KVM. The paper provides details of
   several optimizations, and performance shows results show
   significant improvement for a nested guest operating system
   relative to first-level guest operating system.

   Overall this paper is organized well, but it is very hard to
   read. The paper needs significant editing for grammar, wording, and
   some organization (like the ordering and placement of figures and
   tables). The authors do a reasonable job motivating the problem
   they are trying to solve, although it is not clear whether the
   solution they offer addresses their motivation for running Windows
   XP inside Windows 7. The need for nested virtualization to do
   hypervisor debugging and monitoring is an interesting one, but
   probably not very compelling.

   The claim that hardware virtualization support is required to
   achieve good performance is too broad. Performance of applications
   in a virtualized environment depend on several factors, including
   the VMM, the guest OS, and that application itself. There are some
   situations where hardware support actually degrades performance.

   The background information on hardware support for virtualization
   and KVM provides a reasonable amount of technical detail. The
   description of the nested virtualization design in Section 3 is
   hard to follow, but the techniques proposed all seem relatively
   straightforward.

   The performance evaluation is limited to SPEC benchmarks, and the
   discussion of the results is somewhat limited. It would be very
   interesting to know why the gcc, milc, soplex, and tonto benchmarks
   behave as they do. As is, there is little insight that is
   communicated by the performance evaluation section.

   The lack of related work in nested virtualization is somewhat
   surprising, as this does not seem like a relatively novel
   concept. The introduction cites several examples of the benefits
   for nested virtualization, so it is surprising that there is no
   related work associated with these projects. The paper could also
   be improved by discussing how general the proposed solution is. It
   is probably safe to assume that the proposed mechanisms would work
   for AMD and other VMMs, but some discussion of that would be nice.
