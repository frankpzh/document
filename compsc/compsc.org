#+TITLE: CompSC: Live Migration with pass-through devices
#+LaTeX_CLASS: usenix
#+STARTUP: showall
#+OPTIONS: author:nil
#+OPTIONS: toc:nil

#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \newcommand{\us}{\,$\mu$s\xspace}

#+LATEX_HEADER: \author{
#+LATEX_HEADER: \authname{Zhenhao Pan}
#+LATEX_HEADER: \authaddr{Tsinghua University}
#+LATEX_HEADER: \authurl{\url{frankpzh@gmail.com}}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \authname{Yaozu Dong}
#+LATEX_HEADER: \authaddr{Intel Corp.}
#+LATEX_HEADER: \authurl{\url{eddie.dong@intel.com}}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \authname{Yu Chen}
#+LATEX_HEADER: \authaddr{Tsinghua University}
#+LATEX_HEADER: \authurl{\url{yuchen@tsinghua.edu.cn}}
#+LATEX_HEADER: }

#+LATEX: \begin{abstract}

In recent virtualization techniques, the performance of network I/O is
critical. However, efforts to improve device performance by granting
virtual machines direct access to hardware currently prevent live
migration. None of the existing method can solve this issue
gracefully. This is because current hypervisor can hardly migrate the
states of the device directly, and device drivers in virtual machine
did not help the hypervisor to clone device states efficiently.

This paper introduces CompSC, a mechanism of state cloning to enable
the live migration support with pass-through devices, and we applied
it to an SR-IOV network card. CompSC classified three kinds of states
in pass-through devices, and cloned them with corresponding
techniques. Our experiments show that CompSC enabled the live
migration with less than 2.88\% overhead on the pass-through network
device. Besides, the service downtime during live migration was 35.7\%
less than paravirtualized device.

#+LATEX: \end{abstract}

*Keywords*: Virtualization, Live migration, Pass-through device, State
cloning

* Introduction
  Virtualized systems have recently been widely used. In data centers
  and cloud computing environments, virtualization technology largely
  reduced the hardware costs and resource
  costs\cite{view-cloud,berkeley-cloud,hpc-case}. In such an
  environment, not only the performance of CPU and memory
  virtualization, but also performance of network I/O virtualization
  are highly demanded.

  The technique of virtual machine live migration\cite{lm}
  significantly increases the manageability of virtualized
  systems. When the total demand of CPU resources and I/O resources
  decreases, the system manager can move the virtual machines to empty
  and shut down some physical machines completely. When the CPU or I/O
  demand on a physical machine increases, part of virtual machines on
  it can be moved out to ensure the quality of services.

  In recent researches on virtualization, CPU and memory
  virtualization have been discussed in depth\cite{compare-vt}, and the
  performance with recent techniques is near native
  one\cite{xen-art,mem-manage}. But I/O device virtualization is still
  a challenge, especially on network devices. Emulated network
  I/O\cite{vmware-io} provides a complete emulation of the existing
  network devices, and connects physical network device together with
  a virtual bridge. Paravirtualized (PV) network I/O, which uses a
  simplified device model, has benefited both safety\cite{safe-hw-xen}
  and performance compared to emulated network I/O. Although recent
  efforts\cite{opt-net,bridge-gap-sw-hw} largely improve the performance of
  PV network I/O, it still has a performance gap with native network
  I/O\cite{diag-perf-xen,opt-net,bridge-gap-sw-hw} together with high CPU
  overhead. Using pass-through I/O\cite{bypass-io,vtd}, aka direct
  I/O, virtualized system can directly access physical device without
  interceptions of the hypervisor, thus it can provide a near native
  performance. Single Root I/O Virtualization (SR-IOV)\cite{sriov} is
  a recent specification raised by PCI-SIG. This specification shows
  how PCIe device can share a single root I/O device to several
  virtual machines. With its help, one hardware device can provide
  number of PCIe interfaces to the hypervisor. By assigning these
  interfaces to virtual machines as pass-through devices, the
  scalability of pass-through I/O can be largely increased.

  Nevertheless, pass-through I/O has limitations on virtual machine
  live migration. If using pass-through I/O, the physical device is
  totally controlled by the virtual machine, and its internal state is
  not available to the hypervisor. Several efforts have been made on
  enabling pass-through network I/O
  migration\cite{lm-direct-io,bonding,npia-intel}, but none of them
  solves the problem perfectly.

  In this paper, we introduce CompSC, a mechanism which can
  efficiently clone the state of a hardware device, and enable
  migration with passthough network device. Unlike other researches,
  which bypass the problem of state cloning and use paravirtualized
  device or emulated device as backup, we focused specifically on the
  real issues of the hardware state cloning. With an analysis on
  different kinds of hardware states, CompSC performed methods of
  state replay and self-emulation on pass-through devices. With a
  little code changes on the device driver, CompSC achieved our object
  perfectly with little performance impact at runtime, and little time
  cost for state cloning. Furthermore, compare to the other
  solutions\cite{lm-direct-io,bonding,npia-intel}, CompSC provided
  accuracy on statistic registers.

  We implemented CompSC on Xen\cite{xen} platform, with Intel 82576
  SR-IOV\cite{sriov,sriov-xen} network card\cite{kawela}. In the
  evaluation section, we measured the performance impact of our method
  using micro benchmarks, scp, netperf\cite{netperf} and
  SPECweb2009\cite{specweb}. The result shown that the performance
  impact of our method is within 2.88%. Without the self-emulation
  layer in the hypervisor, the overhead could be as small as 0.75%. We
  also measured the service downtime during migration with a
  paravirtualized network device, Intel 82576 with our implementation,
  and Intel 82576 with the solution of bonding driver. Our solution
  had a 35.7% shorter downtime than the paravirtualized network
  device. Finally, with a line count on code changes in our
  implementation, our solution is easy to deploy and can be widely
  used.

  The remainder of this paper is structured as follows: Section 2
  introduces the related work. Section 3 discusses the hardware
  states, classifies them into three kinds, and explains how did we
  clone each kind of them. Section 4 describes the design of CompSC
  and the structure of the system. Section 5 describes how we
  implement CompSC on Xen platform with Intel 82576 network
  card. Section 6 shows the results of our experiments and Section 7
  is the conclusion.

* Related work
  There have been several efforts on this topic. In the study by Edwin
  et al.\cite{bonding}, Linux Ethernet Bonding
  Driver\cite{bond-drv} is used. Not only the pass-through network
  device, a paravirtualized network device is also used as a
  backup. Before the start of a migration, the pass-through device was
  hot unplugged using an ACPI event. In this way, there is no need to
  worry about migrating the pass-through device. This method does not
  require code change on the guest kernel, but it only works with
  Linux guest. Furthermore, it requires an additional paravirtualized
  network device. The physical device of the additional device must be
  connected to the same switcher with pass-through device. This may
  lead to additional hardware cost and resources cost. In addition,
  the hot plug of the pass-through device after the migration clears
  every statistic registers in the device. Thus the function of
  statistic becomes inaccurate or disabled. In a similar work by Asim
  and Michael\cite{lm-direct-io}, a shadow driver is implemented to
  redirect network requests to a backup device during the
  migration. It can be applied to any operation systems, but it
  requires as large as 11 KLOC change on both the hypervisor and the
  guest kernel.

  Network Plug-In Architecture (NPIA/NPA)\cite{npia-intel,npa} is an
  architecture raised by VMware and Intel. Instead of supporting all
  pass-through network devices, NPIA only focuses on
  SR-IOV\cite{sriov} network devices. NPIA designs a shell-plugin pair
  inside the kernel of the virtual machine. The shell provides a layer
  similar to hardware abstract layer, while the plugin implements the
  hardware communication under the shell. The plugin can be plugged or
  unplugged during runtime. To reduce the downtime during plugin
  switch, an emulated network interface is used as a backup. By
  unpluging the plugin, NPA can easily support live migration. Just
  like the solution of bonding driver, NPIA uses an emulated interface
  as a backup. Compare to the bonding driver solution, NPIA may need
  less time on switching the pass-through device to the backup. NPIA
  also need to completely rewrite the network drivers. This limitation
  maight prevent NPA from being widely used.

* Discussion on hardware states
  In micro view of a hardware, all internal states are registers.
  Every flip-flop is considered as one bit inside the hardware. If we
  can copy the state of every flip-flop in one hardware into another,
  we can easily do the migration with this hardware. Unfortunately, in
  most cases, copying every flip-flop is impossible.

  Hardware specification describes every detail about the interface
  between the device and the driver, together with the communication
  method and the hardware behavior. If we know the past communications
  on the interface, we also know which state the hardware is in. In
  most cases, we can drive the destination hardware from uninitialized
  into the same state by replaying the past communications. Section
  3.1 describes the details of cloning hardware states with state
  replay method. Focusing on the interface itself, hardware typically
  provides I/O registers to software. Some of them are read-write,
  others of them are read-only, and the rest may have uncommon
  attributes such as read-clear. All states represented by read-write
  register can be copied, however, other registers are not
  replicable. Section 3.2 presents an analysis of states exposed by
  read-write registers.

  Also, a set of hardware states cannot be simply cloned even using
  the knowledge of the devices. In states of network devices,
  statistic registers that are exposed read-only or read-clear are in
  this set. These registers can only be altered by real events such as
  receiving a packet. In Section 3.3, we present the ideas of cloning
  this kind of state.

** State replay
   Most of states in a hardware device can be migrated with
   information of past communications. Apparently, the driver must
   know past communications on the hardware-software interface as well
   as the hardware specification. So, the driver is the best one to
   commit the replay of communications and drive the destination
   hardware into the state of source hardware.

   With state replay, the complexity of the driver may be a
   problem, as recording every past communications needs great
   efforts, driving the destination device may also need large amount
   of codes. But, with the knowledge of devices, large amount of
   communications can be optimized. For example, the device driver may
   write a register many times. We did not need to record them
   all. Instead, we recorded the last one, because only the last one
   is valid.

   Another efficient optimization is defining transaction. Some
   driver's work may consist of several device operations. Instead of
   recording every step in the driver's work, we packed the device
   operations into transactions. We assumed that a migration could
   only happen within states outside the transactions. Figure
   \ref{fig:pack_state} illustrates this optimization. In the figure,
   we packed 4 operations =op1=, =op2=, =op3= and =op4= into one
   transaction =tran1=. With the assumption that a migration won't
   happen inside transactions, we safely omitted three states: =A=,
   =B= and =C=.

#+CAPTION: Packing device operations into a transaction
#+LABEL: fig:pack_state
#+ATTR_Latex: width=1.73in
[[./pack_state.eps]]

   This optimization worked dramatically well on the network
   devices. With well-designed transactions, the state set of network
   devices could be largely reduced. In the case of virtual function
   of Intel 82576 network card, which was used in our evaluation, we
   packed all initializing operations and sending/receiving operations
   into transactions. The states remaining were only (uninitialized,
   up, down) together with a bunch of setting registers. In such a
   situation, only the latest operation on each setting registers and
   whether the interface is up are needed to be tracked. Also, the
   code for driving the destination hardware into the state of source
   hardware could be simplfied significantly by invoking existing
   initializing codes.

   Avoiding migration happening inside a transaction needs a
   synchronizing method between the device driver and the hyervisor. A
   common question is: does this affect the performance? This depends
   on the granularity of transactions. If the driver makes a
   transaction which can last for two minutes, we can imagine the
   migration may take a long time. Also, if the driver makes a
   transaction which can be invoked millions of times per second, it
   will be a problem. With a set of well-defined transactions, the
   impact on performance can be minimized. In Section 6.3, we prove
   that the performance impact was small enough.

** I/O registers cloning
   I/O registers are the main interface between hardware and software
   since the born of computer. Almost every visible state of a
   hardware is exposed by kinds of I/O register. In modern PCI
   architecture, three kinds of I/O register are used: Port I/O(PIO),
   Memory-mapped I/O(MMIO), and PCI configuration
   space. Reading/writing PIO and MMIO are atomic, or stateless. In
   other words, the hypervisor can stop the virtual machine at anytime
   and commit PIO/MMIO reading/writing on a pass-through device
   without any difficulties. Operations on PCI configuration consist
   of several PIO operations. However, PCI configuration space of
   virtual machine is totally emulated by the hypervisor. Cloning it
   is never a problem.

** Self-emulation
   Statistic registers exposed with attributes of read-only and
   read-clear often can hardly be cloned through the software/hardware
   interface. The count of dropped packets in network card is an
   example. The only way to alter the count is trying to drop a
   packet. It is difficult, for it needs cooperation from the one on
   the other side of the network wire. All the existing
   solutions\cite{lm-direct-io,bonding,npia-intel} do not cover this
   register. They all do the device initialization after the
   migration, reset all statistic registers, and make the functions of
   statistic inaccurate or disabled.

   Statistic registers often have mathematical attributes. A common
   one is monotonicity. After a migration, one statistic register may
   have an incorrect value. The difference between its value and the
   right value should be a constant. For example, let's assume the
   count of dropped packets was 5 before the migration. After the
   migration, the same register on destination hardware was
   initialized to 0. After that, the value of register was always
   smaller than the right value by 5. If the value on the destination
   hardware was 2, the right value should be 7. Two packets dropped on
   destination machine and seven dropped on the source machine. In the
   case of read-clear register, the relationship is similar. The
   difference is that only the first access to a read-clear register
   after a migration might get the incorrect value.

   With such a clear logic, emulation was choosed. In the method of
   self-emulation, every access to a read-only or read-clear statistic
   register was intercepted by a self-emulation layer. In the layer,
   the right value was calculated and returned to the caller. The
   self-emulation layer could be put in any components on the access
   path of the register (e.g. the driver, the hypervisor). Figure
   \ref{fig:selfemu} represents an example where the self-emulation
   layer is in the hypervisor.

#+CAPTION: An example structure of self-emulation
#+LABEL: fig:selfemu
#+ATTR_Latex: width=2.8in
[[./selfemu.eps]]

** Choices and combination
   I/O register cloning was easy to perform, but it only worked on
   states exposed by read-write registers. State replay covered almost
   every state, but it needed code changes in the driver. Statistic
   registers that are hard to clone were covered by
   self-emulation. One practical way is mixing them into a
   combination: using I/O register cloning if possible, otherwise,
   using state replay and self-emulation.

   In our case, we classified the states of Intel 82576 virtual
   function as follows: Configurations of rings such as RDBA (Receive
   Descriptor Base Address), TXDCTL (Transmit Descriptor Control) were
   cloned using I/O register cloning. Interrupt related registers and
   settings inside Advanced Context Descriptor were cloned using state
   replay. All statistic registers were cloned using self-emulation.
   With these methods, the migration of network cards in our
   experiment ran smoothly.

* Design of CompSC
  Among the five stages of live migration\cite{lm}, CompSC worked
  inside stop-and-copy stage and activation stage. Basically, CompSC
  saved states of the network device at stop-and-copy stage, and
  restored them at activation stage. The architecture of CompSC is
  presented in Figure \ref{fig:arch}.

#+CAPTION: CompSC architecture
#+LABEL: fig:arch
#+ATTR_Latex: width=3in
[[./arch.eps]]

  CompSC used three methods to clone the device states. Before the
  migration, the driver and the hypervisor collected data using these
  methods. After the migration, the restoration of the device states
  was totally completed by the driver using collected data.

  Making the least code changes is one of CompSC's principles.
  Paravirtualized network device\cite{pv} needs two chunks of codes
  working together to achieve the migration: One is the front-end
  driver, and the other is the back-end driver. Emulated network
  device\cite{vmware-io} has another pair, which consists of the
  emulator and the device driver. To avoid making up hundreds of
  "back-end" chunks of codes, in our solution, the hypervisor and
  virtual machine management tools did not have any device-specific
  knowledge. Everything related to the knowledge of devices was
  embedded in the network driver in the virtual machine.

** Synchronization
   As far as the device driver is concerned, device migration happened
   in a flash. After a context switch, the hardware turned into
   uninitialized state. If anything indicated the migration, it must
   be checked before any hardware access. If we defined a set of
   transactions, they would never expect the disturbance of the
   migration.

   CompSC created a shared memory area between the hypervisor and the
   virtual machine. An rwlock and a version counter were presented on
   the memory area. The rwlock indicated the status of migration. When
   the stop-and-copy stage started, the hypervisor tried to hold the
   write lock. In the activation stage, hypervisor added the version
   counter and released the write lock. On the other side, the driver
   acquired the read lock before every hardware access. As soon as the
   lock was held, the driver checked the version counter to figure out
   whether a migration has just happened. If so, the restoration of
   device driver would be invoked. In this way, the hardware would
   never be accessed in an uninitialized state.

   The logical meaning of the rwlock is the indicator of the one who
   took over the hardware device. The device driver locked the read
   lock whenever it wanted to access the hardware. When it finished
   and the device state could be taken over by the hypervisor for
   migration, the driver unlocked the read lock. The hypervisor
   acquired the write lock before it touched the device. When the
   write lock was held by the hypervisor, the hardware device was
   taken over by the hypervisor.

** I/O registers cloning
   CompSC did the I/O register cloning easily. The hypervisor scanned
   a list of registers of the network device, and saved them into the
   shared memory area. After the migration, the driver inside the
   virtual machine would be responsible for restoration. To avoid
   having any device-specific knowledge, the hypervisor did not know
   the list of registers. It got the list from the shared memory area,
   where the driver put the list during boot process.

** State replay
   The state replay was completed in the device driver. The
   transactions and hardware operations were protected by
   rwlock. Every time before the driver released the read lock, it
   stored enough information of the operation or transaction just
   finished for the migration. In the restoration procedure, the
   device drove the destination hardware into the same state using the
   saved information.

** Self-emulation layer
   Self-emulation layer could be put into the hypervisor or the device
   driver. A self-emulation layer in the hypervisor would trap every
   access to the emulated registers, and return the right value. A
   self-emulation layer in the driver would process the fetched value
   right after the access. The former needed less code changes in the
   driver. All it needed was the list of emulated registers. But it
   led to performance impact due to I/O interception. The latter
   gained less overhead, but much more code changes. CompSC provided
   them both, and the driver was free to choose any one. For the
   overhead of I/O interception, the detail will be decscribed Section
   6.1.

** SR-IOV network card
   It would be different when using SR-IOV network device. An SR-IOV
   network device consists of one physical function (PF) and several
   virtual functions (VFs). The typical usage of an SR-IOV network
   device on virtual machine is taking VFs as pass-through devices of
   virtual machines, and taking PF as a device of device domain or
   privileged domain, not only for networking, but also for VF
   management. On PCI bus, a VF looks identical to an independent PCI
   device. Also, in a virtual machine, pass-through VF is just like a
   typical PCI network card.

   VFs are managed by the PF, thus states of VFs can also be affected
   by the PF. Furthermore, some of the states can only be accessed
   through PF registers by the PF driver. When a migration happens,
   the VF part of PF states (VF-in-PF states) should also be saved and
   restored. CompSC used the state replay method directly on the PF
   driver. The PF would record all states about the specified VF
   before the migration, and redo them on the destination machine
   later.

* Implementation
  We used Xen\cite{xen} as the base of our implementation. For
  architecture, we used 64-bit x86. For network card, we used Intel
  82576, an SR-IOV 1Gbps network card. The PF driver and the VF driver
  of Intel 82576 were changed in our implementation. Section 5.1
  describes the detail of driver changes, and Section 5.3 presents the
  self-emulation layer.

  Xen provids functions in the hypervisor to access foreign guest
  domain's memory page. Using these functions, shared pages between
  the hypervisor and the device driver can be well
  implemented. Section 5.2 describes the details.

  The process of live migration highly depends on dirty page
  tracking. Dirty page tracking is implementated with the help of page
  tables in the newest version of Xen. However, memory access by DMA
  could not be tracked by page tables. Intel VT-d technology\cite{vtd}
  provides I/O page tables, but it still cannot be used to track dirty
  pages. Section 5.4 discusses our solution to dirty page tracking.

** Driver changes
   Like the description in Section 4.1, the read lock of the rwlock
   was used to protect the hardware operations and the transactions we
   defined. Right after the lock was acquired, the driver checked the
   migration counter. The driver invoked restoration procedure if a
   migration just happend.

   To be specific, we packed the =igbvf_up= and =igbvf_down= as
   transactions. All the hardware operations and transactions are
   protected by the read lock. Most of device states had a copy in the
   driver, the state replay needed little code changes. The
   restoration procedure did the following tasks: initializing the
   device, writing all saved registers, and restoring all states using
   state replay.

** Shared page and synchronization
   Shared pages were allocated by the network device driver. The
   driver allocated several continuous pages and put three contents
   into these pages:

   * The rwlock and the version counter;
   * The list of registers that should be saved in the migration;
   * The list of counter registers that need the help of
     self-emulation layer in the hypervisor.

   After the initialization, the GFN (guest frame number) of the first
   page was sent to the hypervisor. In our implementation, this number
   was sent by PF-VF communication. For non-SR-IOV network card, this
   number could be sent by a high level communication on TCP/IP
   protocol.

   When a live migration started, it kept transfering memory pages
   until the stop-and-copy stage\cite{lm}, and then tried to suspend
   the virtual machine. Right before the suspending, the write lock of
   the rwlock was acquired by the hypervisor. In this way, the
   hypervisor took over the control of the device hardware. After the
   virtual machine was suspended, the hypervisor accessed the shared
   pages, and saved all registers listed in the shared pages. The
   remaining part of live migration happend on the backup
   machine. Before the hypervisor tried to resume the virtual machine,
   saved values of read-only and read-clear counter registers were
   sent to the self-emulation layer in the hypervisor.

   At the first time when the driver acquired the read lock, device
   restoration procedure was invoked. The driver did necessary
   initializations on the device and restored the state using
   information collected by state replay and I/O register
   cloning. After that, the device migration was accomplished
   perfectly.

** Self-emulation layer
   Xen hypervisor provides functions for trapping memory accesses. The
   self-emulation layer in the hypervisor was based on them. Every
   time the layer received a request to commit self-emulation on a
   list of registers, it placed a mark on the page table of the
   register. All the further accesses to these registers would be
   trapped and emulated. The emulation did the real MMIO, and the
   layer returnd the calculated value to the virtual machine. The
   granularity of trapping in our implementation was one page. In
   64-bit x86, that is 4 KB. This might lead to unnecessary trappings
   and performance impacts. In Section 6.3, we elaborate the
   performance impact.

** Pages dirtied by DMA
   It is difficult to mark a page written by hardware as dirty
   automatically, while marking it manually is simple. All we need is
   doing a memory write. In a typical network device, hardware
   accesses descriptor rings and buffers by DMA. After the hardware
   wrote anyone of them, an interrupt would be sent to the driver in
   the guest kernel. The driver knew all changes on the descriptor
   rings and buffers, so it could do dummy writes (read a byte and
   write it back) to mark the pages as dirty.

   This method missed a little number of packets that had already been
   processed by the hardware but had not been processed by the driver
   yet. This might lead to packets duplicating or packets
   missing. Fortunately, the amount of such packets would not impact
   connections of reliable protocols such as TCP connections. Section
   6.2 presents the details of these duplicated or missed packets.

** Descriptor ring
   During our implementation, we came across an issue on Intel 82576
   VF. The head register of descriptor rings (either RX and TX) are
   read-only. The values of them are owned by the hardware, and
   writing any value except for 0 is not allowed (writing 0 is an
   initialization). Thus, head registers can only be restored using
   state replay method.

   One method to solve it was resetting everything in the rings. By
   freeing buffers in rings and resetting rings to empty, the driver
   would work well with the device. But this method needed tens or
   hundreds of memory allocations and freeings. The time cost might be
   a problem especially when the device had a large ring.

   Another idea was shifting. Instead of restoring the value of head
   registers, we shifted the ring itself. During the restoration
   procedure, the driver shifted the RX and TX rings, and made sure
   the position of each original head was at index 0. After that, the
   driver only needed initialization on head registers to make the
   rings work. Also, the driver saved the offsets between the original
   rings and the shifted rings. Every time the head/tail registers or
   rings were accessed by the driver, the offsets were used to make
   sure the access was right. This method introduced additional
   operations on accessing indexes/rings, so it consumed more time in
   the driver. Section 6.3 will measure this performance impact.

* Evaluation
  In this section, we present the performance data with our
  implementation of CompSC and compare them to the system without
  CompSC (original one) and the bonding driver solution. We first
  present a micro benchmark to measure the performance impact due to
  self-emulation layer in the hypervisor. Then we show our measurement
  on the number of duplicated or missed packet due to DMA dirty page
  issue in Section 6.2. With scp, netperf and SPECweb2009 benchmark,
  Section 6.3 presents a comparison of the runtime performance between
  several situations including the original environment and our
  implementation. Section 6.4 illustrates the migration process using
  a timeline figure, with CompSC, paravirtualized device, and bonding
  driver solution. In the end, Section 6.5 lists the amount of code
  changes during our implementation.

  The evaluation used the following environment: two equivalent
  servers, with Intel Core i5 670 CPU (3.47 GHz, 4 cores), 4 GB
  memory, 1 TB harddisk, and Intel 82576 SR-IOV network card; one
  client machine for SPECweb2009 client, with Intel Core i3 540 CPU
  (3.07 GHz, 4 cores), 4 GB memory, 500 GB harddisk and an Intel
  82578DC network card. These three machines were connected using a
  1000 Mb network switcher. The virtual machine used 4 virtual CPUs, 3
  GB memory, and a virtual function of Intel 82576 network card. It
  was virtualized in HVM. The virtual machine also used a PV network
  device in the tests with PV device.

** Micro benchmark for self-emulation
   In Section 3.3 we presented our idea of self-emulation, and figured
   out that the idea is a tradeoff between accuracy and
   performance. In this section we measure the performance loss due to
   self-emulation. In our test, we access one of the counter registers
   10,000 times. Using TSC register, we measured the total cost of CPU
   cycles and got the average. We ran our test in both the
   direct-access situation and the intercepted situation. Table
   \ref{tbl:mmio} represents the results.

#+CAPTION: Micro benchmark for MMIO cost
#+LABEL: tbl:mmio
#+ATTR_Latex: align=|l|l|
   |---------------+------------------|
   | *MMIO direct* | *MMIO intercept* |
   |---------------+------------------|
   | 3911 cycles   | 11860 cycles     |
   |---------------+------------------|

   These results show that the MMIO cost with interception needed
   additional 7,949 cycles for =VMEnter/VMExit= and context
   switches. For low access frequency, this overhead was
   ignorable. But for high access frequency, the overhead might become
   a problem.  Next, we measured the access frequency of statistic
   registers in different workloads.

#+CAPTION: Access rate of statistic registers
#+LABEL: tbl:mmio_rate
#+ATTR_Latex: align=|l|l|l|l|l|
   |---------+---------+------------+------------+--------|
   |         | *Time*  | *Rx bytes* | *Tx bytes* | *MMIO* |
   |---------+---------+------------+------------+--------|
   | Netperf | 60.02 s | 54.60 G    | 1.19 G     | 4.50/s |
   |---------+---------+------------+------------+--------|
   | SPECweb | 8015 s  | 8.55 G     | 294.68 G   | 4.50/s |
   |---------+---------+------------+------------+--------|

   Table \ref{tbl:mmio_rate} shows the access frequency of statistic
   registers. In the result, we figure out that the frequency of
   statistic register access was a constant: 4.5 access/s, no matter
   what task it was performing, and no matter Rx and Tx which one is
   heavier. A following code check on the linux kernel uncovered this
   behavior. IGBVF driver uses a watchdog with a frequency of 0.5 Hz
   to observe the statistic registers, and the access frequency should
   be a constant. At such low frequency, the overhead of
   self-emulation was 10.30\us/s. With consideration of cache and TLB,
   the overhead might be slightly bigger. But, this overhead can still
   be considered small.

** Duplicated and missed packet due to unmarked dirty page
   In Section 5.4, we presented our idea of marking pages dirtied by
   DMA. The solution might cause packet loss and packet
   duplication. In this section, we measured the number of duplicated
   packets and missed packets under different workloads. A
   straight-forward prediction was that the number might become larger
   when the network device is busy. In our measurement, the workload
   of scp and SPECweb were used, and the situation of no workload was
   also considered.

#+CAPTION: Duplicated and missed packet count during live migration
#+LABEL: tbl:miss_pkt
#+ATTR_Latex: align=|l|l|l|
   |-------------+-------+--------|
   |             | *Dup* | *Miss* |
   |-------------+-------+--------|
   | No workload |     0 |      0 |
   |-------------+-------+--------|
   | scp         |     0 |      0 |
   |-------------+-------+--------|
   | SPECweb     |     0 |      3 |
   |-------------+-------+--------|

   The results in Table \ref{tbl:miss_pkt} show that, our method
   worked perfectly on both no worload situation and scp situation. No
   packet loss or duplication was happened. On SPECweb workload, only
   3 packet losses and no packet duplication were happend. These
   abnormal behaviors will not break the connection of TCP, and thus
   the service kept live during the migration.

** Performance with workloads
   CompSC added a synchronization method between the hypervisor and
   the driver. Performance impact of this addition was a critical data
   of our solution. The method described in Section 5.5 also had
   performance impact at runtime. In this section, the runtime
   performance of CompSC is measured and compared to original one. The
   self-emulation layer in the hypervisor also had performance
   overhead. Although in the test of Section 6.1, the overhead was
   measured as small, we still consider this factor in this
   section. Also, in Section 5.3 we described the layer might perform
   unnecessary interceptions. The layer was optional and was only
   enabled after migration, so the situation with and without the
   layer are both measured.

   The first test ran a benchmark of Netperf, and an scp workload with
   a CD image file =specweb2009.iso= sized 491.72 MB. In this test we
   measured the throughput of the workload in four situations: Domain
   0 (Dom0), original IGBVF driver (VF orig), IGBVF driver with CompSC
   (VF+comp), and IGBVF driver with CompSC and with self-emulation
   layer enabled (VF+comp+int). Figure \ref{fig:perf_tp} illustrates
   the results. In the figure, we can see that the throughput of four
   situations are almost the same in two workloads. Also, the CPU
   utilizations in the figure presents that the VF+comp and
   VF+comp+int situations consumed almost the same amount of CPU
   resources as VF orig situation. The CPU utilization of Domain 0
   differed from three VF situations, because they had different
   kernel version, linux distribution, and background processes. The
   only thing we notice is that the throughput of scp on VF+comp+int
   was slightly less than that on VF orig and VF+comp. On Netperf
   benchmark, the network was the bottleneck of the whole system while
   on scp workload, CPU was the bottleneck. The CPU utilizations near
   100 percents show a CPU bottleneck of a single-threaded
   workload. The situation with self-emulation layer consumed more CPU
   resources and thus had a slightly lower performance.

#+CAPTION: Throughput and CPU utilization by scp and Netperf
#+LABEL: fig:perf_tp
#+ATTR_Latex: width=\linewidth
[[./perf_tp.eps]]

   SPECweb 2009 is our real-world benchmark. In our evaluation, we
   configured and ran SPECweb 2009 with different pressuresx on the
   server in the virtual machine. We invoked the test with five
   different configurations, each with 50, 100, 150, 200, 250
   concurrent sessions respectively. Also, the tests were ran above
   three situations: original IGBVF driver (VF orig), IGBVF driver
   with CompSC (VF+comp), and IGBVF driver with CompSC and with
   self-emulation layer enabled (VF+comp+int).

   SPECweb 2009 classifies the requests based on response time into
   three types: good ones, tolerable ones, and failed ones. The good
   ones are requests which have a quick response, while the tolerable
   ones have a long but tolerable response time. Failed ones have
   intolerable response time, or no response at all. In our test, we
   collected the number of good requests and presented them in Figure
   \ref{fig:perf_spec_req}.

#+CAPTION: Good requests by SPECweb 2009
#+LABEL: fig:perf_spec_req
#+ATTR_Latex: width=\linewidth
[[./perf_spec_req.eps]]

   The number of good requests lifted when the number of sessions is
   increasing linearly, until we met the bottleneck at 250
   sessions. To uncover the bottleneck clearly, we also represents the
   average response time of requests in Figure
   \ref{fig:perf_spec_resp}. The average response time was on the same
   horizontal line when the number of sessions was less than 250. On
   the test with 250 sessions, the response time growed almost 2/3,
   indicating clearly that the server was in a heavy workload.

#+CAPTION: Average response time by SPECweb 2009
#+LABEL: fig:perf_spec_resp
#+ATTR_Latex: width=\linewidth
[[./perf_spec_resp.eps]]

   Before reaching the bottleneck, no obvious differences were found
   in the three situations in Figure \ref{fig:perf_spec_req} and
   Figure \ref{fig:perf_spec_resp}. This convinced that the
   performance impact of our method under light workload can be simply
   ignored. When the test approaches 250 sessions, VF+comp generated
   3.74% less good requests than VF orig, and VF+comp+int generated
   6.80% less good requests (in Figure \ref{fig:perf_spec_req}). On
   the measurement of average response time, VF+comp had 0.75% more
   response time and VF+comp+int had 2.88% more (in Figure
   \ref{fig:perf_spec_resp}). To figure out the reasons, we collect
   the detailed performance data and CPU utilization with 250 sessions
   in Figure \ref{fig:perf_spec_250}.

#+CAPTION: Performance and CPU utilization by SPECweb 2009 with 250 sessions
#+LABEL: fig:perf_spec_250
#+ATTR_Latex: width=\linewidth
[[./perf_spec_250.eps]]

#+BEGIN_LaTeX
\begin{figure*}[htb]
\epsfig{file=timeline_compsc.eps}
\caption{CompSC: Throughput and CPU utilization during live migration}
\label{fig:timeline_compsc}
\end{figure*}
#+END_LaTeX

   The total requests handled by the server in three situations were
   on the same horizontal line in Figure \ref{fig:perf_spec_250}. The
   reason why VF+comp and VF+comp+int had less good requests is the
   longer response time. Some of the requests were classified into
   tolerable requests because they had longer response time. In other
   words, VF+comp and VF+comp+int situation had the same service
   capability, but had slight longer response time. In the meantime,
   VF+comp and VF+comp+int consumed 0.59% and 0.64% more CPU
   respectively, whose impact can also be considered as very small.

** Service down time
#+BEGIN_LaTeX
\begin{figure*}[htb]
\epsfig{file=timeline_pv.eps}
\caption{PV device: Throughput and CPU utilization during live migration}
\label{fig:timeline_pv}
\end{figure*}
#+END_LaTex

#+BEGIN_LaTeX
\begin{figure*}[htb]
\epsfig{file=timeline_bond.eps}
\caption{Bonding driver: Throughput and CPU utilization during live migration}
\label{fig:timeline_bond}
\end{figure*}
#+END_LaTex

   In this section, we illustrates the whole process of live
   migration. We treated the server as live if it had a negative
   throughput. To fullfil the throughput, we ran Netperf benchmark
   during our test. The throughput on the Netperf client machine was
   recorded as data. To shorten the migration time, which was mostly
   decided by the amount of memory, we changed the virtual machine
   configuration. In this test, the virtual machine had 1 GB memory.

   During live migration, the service in the virtual machine should
   remain alive. However, in our environment, the service stopped
   after the migration in the sitations of both PV device and
   CompSC. After an analysis on network packets, we found the root
   cause. The root cause was that the network switcher did not know
   the movement of the virtual machine. It kept forwarding packets to
   the virtual machine's old place after the migration. We changed
   both the IGBVF driver and the Xen ethernet frontend driver to send
   an ARP response after the live migration. As soon as the switcher
   received the ARP packet, it changed its MAC-Port mapping and all
   the incoming packets were forwarded correctly.

   Figure \ref{fig:timeline_compsc} presents the throughput and CPU
   utilization during a live migration in the situation of CompSC, and
   Figure \ref{fig:timeline_pv} presents the result in the situation
   of PV device. In the figures, we first notice that the service
   downtime of CompSC was about 0.9s while the downtime of PV device
   was about 1.4s. CompSC had a 35.7% shorter and better service
   downtime. We also notice that in the test of PV device, service was
   down shortly before the 1.4s downtime (On about 20.6s). In the
   meantime, the CPU utilization went as high as 327%. The reason of
   this behavior was the suspending process of PV-on-HVM virtual
   machine. The suspending on PV-on-HVM needed cooperations of drivers
   in the virtual machine. These cooperations consumed much CPU
   resources and caused a small period of service down. Focusing on
   the CPU utilization line, we notice that the lines on both figures
   have the same shape, and the line on Figure \ref{fig:timeline_pv}
   is higher than the line on Figure \ref{fig:timeline_compsc}. This
   fits our expectation. The pass-through device consumed less CPU
   resources than the PV device, that was the advantage of
   pass-through device.

   We also had a test on the solution of bonding driver. With the
   limitation of current Xen implementation, we only tested the
   bonding driver with a VF of Intel 82576 and an emulated E1000
   device as backup. Figure \ref{fig:timeline_bond} shows the
   result. The solution of bonding driver had an extra service down at
   about 3s. This is because that the switching of bonding driver took
   several milliseconds and caused packet loss. The shape of CPU
   utilization line is similar to that of CompSC and PV device, but
   the throughput was much less. The performance of emulated device
   was not as good as PV device or pass-through device. In the figure,
   we can also get the service downtime of bonding driver solution:
   about 1.2s.

** Implementation complexity
   The CompSC needed code changes in the network device driver. In a
   common doubt on whether it is easy to deploy, the complexity of
   device code changes is the most critical one. In Table
   \ref{tbl:loc}, we collect the line of code changes in our
   implemenation on different components. The synchronization
   mechanism was common to every network driver which is willing to do
   live migration. The total code changes of it was just 220 lines. On
   VF driver, only 183 lines of codes were added or modified. It is
   said that one can easily patch an existing device driver into a
   CompSC supported one. Even the CompSC architecture itself had small
   amount of code changes. Only 854 lines of codes were added or
   modified in both the Xen hypervisor and Xen tools. Thus, the CompSC
   is easy to deploy.

#+CAPTION: Lines of code changes in the implementation
#+LABEL: tbl:loc
#+ATTR_Latex: align=|l|l|
   |-------------------+----------------|
   |                   | *Line of code* |
   |-------------------+----------------|
   | Xen hypervisor    |            390 |
   |-------------------+----------------|
   | Xen tools         |            464 |
   |-------------------+----------------|
   | VF driver(common) |            220 |
   |-------------------+----------------|
   | VF driver(spec)   |            183 |
   |-------------------+----------------|
   | PF driver         |            181 |
   |-------------------+----------------|

* Conclusion
  In this paper we presented CompSC, a state cloning mechanism to
  achieve the live migration support on pass-through network
  devices. During the migration, three kinds of device states were
  cloned using the most appropriate method. With a synchronize
  mechanism between the device driver and the hypervisor, the hardware
  was taken over by the hypervisor and performed register
  saving. Right after the migration, device driver restored the
  hardware state on the destination machine using knowledge of the
  device and register values saved by the hypervisor. Furthermore, a
  self-emulation layer inside the hypervisor was provided to achieve
  the accuracy of statistic registers.

  Our method had less than 2.88% performance impact at runtime, and a
  service downtime 35.7% shorter than that of paravirtualized network
  device during the live migration. Besides, our method needed little
  implementation effort and could be easily deployed on different
  devices.

#+LATEX: \bibliographystyle{unsrt}
#+LATEX: \bibliography{compsc}

* Comments from Middleware                                         :noexport:
  I would have liked some more results related to the robustness of
  implementation, e.g. how many times did you manage to migrate back
  and forth or in a circle around multiple random machines. Also,
  individual migration is easy, it becomes a problem in presence of
  multiple migrations taking place in the system concurrently.

  I would have liked more details on the use of migration. What
  scenarios did you use migration in, how effective it was, etc.

  you write ".. with a like count on code changes in our
  implementation, our solution is easy to deploy and can be widely
  used" This is somewhat subjective statement. First, there is a
  requirement to make code changes of the VM and hypervisor, is this
  true? This somewhat limits deployment and use.

  Please reference appropriately with number of the issue, paegs,
  year/month. (e.g. in reference 1 and elsewhere for
  magazines/journals) Please use year and pages for conference
  proceedings.

  missing discussing of the choice of benchmarks (netperf, scp), what
  kind of load the represent

  on page 1, "Several efforts have been made on enabling pass-through
  network I/O migration[15, 16, 17], but none of them solves the
  problem perfectly": the discussion on shortcomings is fragmented on
  the paper and in some cases not in sufficient depth

  refers to bonding driver in initial part of the paper without
  defining it, so discussion may be lost in he reader

  could provide better arguments on the sync part: added a lock to all
  dev operations. It's true the lock won't be contended in the common
  case, but it would make sense to discuss this when first introduced

  on page 5, replay is discussed, but without any specific information
  on when/which operations were tracked for replay? The ones related
  to read/only and read-clear?

  on page 5, "It would be different when using SR-IOV network device":
  why?

  It would be good if the paper discussed the applicability of this
  approach for other hypervisors

  The evaluation section provides table 3 with info on
  duplicated/missing packages for the scp/specweb workloads.  But what
  kind of workload could have bigger numbers?

  How come the migration setup was not working out of the box, and you
  had to find root cause and fix with the ARP?

  The paper discusses live migration support for pass-through network
  devices. How about applicability for other pass-through devices? The
  last sentence on the paper says that the method requires little
  effort and could be easily deployed on different devices (I guess
  you're assuming other network devices) but no arguments were
  provided to back this statement up

  For instance, one of the main mechanisms to support live-migration
  leverages the replay of past communication to bring the new driver
  to the up-to-date state. A natural question to me seems how scalable
  is this solution. How much state need to be saved to enable state
  replay? The authors mention that some optimizations are possible but
  it would have been nice to see some real numbers and experimental
  analysis.

  both CompSC and the para-virtualized driver achieve the same
  throughput. I can imagine that under more challenging scenarios
  (e.g., a 10-Gbps network interface), the direct I/O driver would
  outperform the para-virtualized one. It would be interesting to see
  in these conditions what are the performance of CompSC.
