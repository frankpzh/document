#+TITLE: CompSC: Live Migration with pass-through devices
#+LaTeX_CLASS: usenix
#+STARTUP: showall
#+OPTIONS: author:nil
#+OPTIONS: toc:nil

#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \newcommand{\us}{\,$\mu$s\xspace}

#+LATEX_HEADER: \author{
#+LATEX_HEADER: \authname{Zhenhao Pan}
#+LATEX_HEADER: \authaddr{Tsinghua University}
#+LATEX_HEADER: \authurl{\url{frankpzh@gmail.com}}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \authname{Yaozu Dong}
#+LATEX_HEADER: \authaddr{Intel Corp.}
#+LATEX_HEADER: \authurl{\url{eddie.dong@intel.com}}
#+LATEX_HEADER: \and
#+LATEX_HEADER: \authname{Yu Chen}
#+LATEX_HEADER: \authaddr{Tsinghua University}
#+LATEX_HEADER: \authurl{\url{yuchen@tsinghua.edu.cn}}
#+LATEX_HEADER: }

#+LATEX: \begin{abstract}

In recent virtualization techniques, the performance of network I/O is
critical. However, efforts to improve device performance by granting
virtual machines direct access to hardware currently prevent live
migration. None of the existing methods can solve this issue
*gracefully*. This is because current hypervisor can hardly migrate
the states of the device directly, and device drivers in virtual
machine could not help the hypervisor to clone device states
efficiently.

This paper introduces CompSC, a mechanism of state cloning to enable
the live migration support with pass-through devices, and we applies
it to an SR-IOV network card. CompSC classifies three kinds of states
in pass-through devices, and clones them with corresponding
techniques. Our experiments show that CompSC enabled the live
migration with less than 2.88\% overhead on the pass-through network
device. Besides, the service downtime during live migration is 35.7\%
less than para-virtualized device.

#+LATEX: \end{abstract}

*Keywords*: Virtualization, Live migration, Pass-through device, State
cloning

* Introduction
  Virtualized systems have recently been widely used. In data centers
  and cloud computing environments, virtualization technology largely
  reduces the hardware and resource
  costs\cite{berkeley-cloud,hpc-case}. In such an
  environment, not only the performance of CPU and memory
  virtualization, but also performance of network I/O virtualization
  are highly demanded.

  The technique of virtual machine live migration\cite{lm}
  significantly increases the manageability of virtualized
  systems. When the total demand of CPU resources and I/O resources
  decreases, the system manager can move the virtual machines to empty
  and shut down some physical machines completely. When the CPU or I/O
  demand on a physical machine increases, part of virtual machines on
  it can be moved out to ensure the quality of services.

  In recent researches on virtualization, CPU and memory
  virtualization have been discussed in depth\cite{compare-vt}, and
  the performance with recent techniques is near native
  one\cite{xen-art,mem-manage}. But I/O device virtualization is still
  a challenge, especially on network devices. Emulated network
  I/O\cite{vmware-io} provides a complete emulation of the existing
  network devices, and connects physical network device together with
  a virtual bridge. Para-virtualized (PV) network I/O, which uses a
  simplified device model, has benefited both safety\cite{safe-hw-xen}
  and performance compared to emulated network I/O. Although recent
  efforts\cite{opt-net,bridge-gap-sw-hw} largely improve the
  performance of PV network I/O, it still has a performance gap with
  native network I/O\cite{diag-perf-xen,opt-net,bridge-gap-sw-hw}
  together with high CPU overhead. Using pass-through
  I/O\cite{bypass-io,vtd}, A.K.A direct I/O, virtualized system can
  directly access physical device without interceptions of the
  hypervisor, thus it can provide a near native performance. Single
  Root I/O Virtualization (SR-IOV)\cite{sriov} is a recent
  specification raised by PCI-SIG. This specification shows how PCIe
  device can share a single root I/O device with several virtual
  machines. With its help, one hardware device can provide number of
  PCIe interfaces to the hypervisor. By assigning these interfaces to
  virtual machines as pass-through devices, the scalability of
  pass-through I/O can be largely increased.

  Nevertheless, pass-through I/O has limitations on virtual machine
  live migration. If using pass-through I/O, the physical device is
  totally controlled by the virtual machine, and its internal state is
  not available to the hypervisor. Several efforts have been made on
  enabling pass-through network I/O
  migration\cite{lm-direct-io,bonding,npia-intel}, but none of them
  solves the problem perfectly.

  In this paper, we introduce CompSC, a mechanism which can
  efficiently clone the state of a hardware device, and enable
  migration with pass-though network device. Unlike other researches,
  which bypass the problem of state cloning and use para-virtualized
  device or emulated device as backup, we focus specifically on the
  real issues of the hardware state cloning. With an analysis on
  different kinds of hardware states, CompSC applies methods of state
  replay and self-emulation on pass-through devices. With a few code
  changes on the device driver, CompSC achieves our object perfectly
  with little impact on performance at runtime, and little time cost
  for state cloning. Furthermore, compared with the other
  solutions\cite{lm-direct-io,bonding,npia-intel}, CompSC maintains
  the values of statistic registers across migrations.

  We have implemented CompSC on Xen\cite{xen} platform, with Intel
  82576 SR-IOV\cite{sriov,sriov-xen} network card\cite{kawela}. In the
  evaluation section, we measure the performance impact of our method
  using micro benchmarks, scp, netperf\cite{netperf} and
  SPECweb2009\cite{specweb}. The result shows that the performance
  degradation of our method is within 2.88%. Without the
  self-emulation layer in the hypervisor, the overhead is as small as
  0.75%. We also measure the service downtime during migration with a
  para-virtualized network device, Intel 82576 with our implementation,
  and Intel 82576 with the solution of bonding driver. Our solution
  has a 35.7% shorter downtime than the para-virtualized network
  device. Finally, with a line count on code changes in our
  implementation, our solution is easy to be deployed and can be
  widely used *?*.

  The remainder of this paper is structured as follows: Section 2
  introduces the related work. Section 3 discusses the hardware
  states, classifies them into three kinds, and explains how did we
  clone each kind of them. Section 4 describes the design of CompSC
  and the structure of the system. Section 5 describes how we
  implement CompSC on Xen platform with Intel 82576 network
  card. Section 6 shows the results of our experiments and Section 7
  is the conclusion.

* Related work
  There have been several efforts on this topic. In the study by Edwin
  et al.\cite{bonding}, Linux Ethernet Bonding Driver\cite{bond-drv}
  is used. Not only the pass-through network device, a
  para-virtualized network device is also used as a backup. Before the
  start of a migration, the pass-through device was hot unplugged
  using an ACPI event. In this way, there is no need to worry about
  migrating the pass-through device. This method does not require code
  change on the guest kernel, but it only works with Linux
  guest. Furthermore, it requires an additional para-virtualized
  network device. The physical device of the additional device must be
  connected to the same switcher with pass-through device. This may
  lead to additional hardware cost and resources cost. In addition,
  the hot plug of the pass-through device after the migration clears
  every statistic registers in the device. Thus the function of
  statistic becomes inaccurate or disabled. In a similar work by Asim
  and Michael\cite{lm-direct-io}, a shadow driver is implemented to
  redirect network requests to a backup device during the
  migration. It can be applied to any operating systems, but it
  requires as large as 11K LOC (line of code) change on both the
  hypervisor and the guest kernel.

  Network Plug-In Architecture (NPIA/NPA)\cite{npia-intel,npa} is an
  architecture raised by VMware and Intel. Instead of supporting all
  pass-through network devices, NPIA only focuses on
  SR-IOV\cite{sriov} network devices. NPIA designs a shell-plugin pair
  inside the kernel of the virtual machine. The shell provides a layer
  similar to hardware abstract layer, while the plugin implements the
  hardware communication under the shell. The plugin can be plugged or
  unplugged during runtime. To reduce the downtime during plugin
  switch, an emulated network interface is used as a backup. By
  unpluging the plugin, NPA can easily support live migration. Just
  like the solution of bonding driver, NPIA uses an emulated interface
  as a backup. Compared with the bonding driver solution, NPIA may
  need less time on switching the pass-through device to the
  backup. NPIA also needs to completely rewrite the network
  drivers. This limitation might prevent NPA from being widely
  employed.

* Discussion on hardware states
  In micro view of a hardware, all internal states are registers.
  Every flip-flop is considered as one bit inside the hardware. If we
  can copy the state of every flip-flop in one hardware into another,
  we can easily do the migration with this hardware. Unfortunately, in
  most cases, copying every flip-flop is impossible.

  Hardware specification describes every detail about the interface
  between the device and the driver, together with the communication
  method and the hardware behavior. If we know the past communications
  on the interface, we also know which state the hardware is in. In
  most cases, we can drive the destination hardware from uninitialized
  into the same state by replaying the past communications. Section
  3.1 describes the details of cloning hardware states with state
  replay method. Focusing on the interface itself, hardware typically
  provides I/O registers to software. Some of them are read-write,
  others of them are read-only, and the rest may have uncommon
  attributes such as read-clear. All states represented by read-write
  register can be copied, however, other registers are not
  replicable. Section 3.2 presents an analysis of states exposed by
  read-write registers.

  Also, a set of hardware states cannot be simply cloned even using
  the knowledge of the devices. In states of network devices,
  statistic registers that are exposed read-only or read-clear are in
  this set. These registers can only be altered by real events such as
  receiving a packet. In Section 3.3, we present the ideas of cloning
  this kind of state.

** State replay
   Most of states in a hardware device can be migrated with
   information of past communications. Apparently, the driver must
   know past communications on the hardware-software interface as well
   as the hardware specification. So, the driver is the best one to
   commit the replay of communications and drive the destination
   hardware into the state of source hardware.

   With state replay, the complexity of the driver may be a problem.
   As recording every past communications needs great efforts, driving
   the destination device may also need large amount of code. But,
   with the knowledge of devices, large amount of communications can
   be optimized. For example, the device driver may write a register
   of a setting many times. We do not need to record them
   all. Instead, we record the last one, because only the last one is
   valid.

   Another efficient optimization is defining transaction. Some
   driver's work may consist of several device operations. Instead of
   recording every step in the driver's work, the device operations
   are packed into transactions. We assume that a migration could only
   happen within states outside the transactions. Figure
   \ref{fig:pack_state} illustrates this optimization. In the figure,
   4 operations =op1=, =op2=, =op3= and =op4= are packed into one
   transaction =tran1=. With the assumption that a migration won't
   happen inside transactions, three states are safely omitted: =A=,
   =B= and =C=.

#+CAPTION: Packing device operations into a transaction
#+LABEL: fig:pack_state
#+ATTR_Latex: width=1.73in
[[./pack_state.eps]]

   This optimization works dramatically well on the network
   devices. With well-designed transactions, the state set of network
   devices can be largely reduced. In the case of virtual function of
   Intel 82576 network card, which is used in our evaluation, all
   initializing operations and sending/receiving operations are packed
   into transactions. The remaining states are only (uninitialized,
   up, down) together with a bunch of setting registers. In such a
   situation, only the latest operation on each setting registers and
   whether the interface is up are needed to be tracked. Also, the
   code for driving the destination hardware into the state of source
   hardware is simplfied significantly by invoking existing
   initializing codes.

   Avoiding migration happening inside a transaction needs a
   synchronizing method between the device driver and the
   hypervisor. A common question is: does this affect the performance?
   This depends on the granularity of transactions. If the driver
   makes a transaction which can last for two minutes, we can imagine
   the migration may take a long time. Also, if the driver makes a
   transaction which can be invoked millions of times per second, it
   will be a problem. With a set of well-defined transactions, the
   decrease on performance can be minimized. In Section 6.3, we prove
   that the performance deterioration was small enough.

** I/O registers cloning
   I/O registers are the main interface between hardware and software
   since the born of computer *?*. Almost every visible state of a
   hardware is exposed by kinds of I/O registers. In modern PCI
   architecture, three kinds of I/O registers are used: Port I/O(PIO),
   Memory-mapped I/O(MMIO), and PCI configuration
   space. Reading/writing PIO and MMIO are atomic, or stateless. In
   other words, the hypervisor can stop the virtual machine at anytime
   and commit PIO/MMIO reading/writing on a pass-through device
   without any difficulties. Operations on PCI configuration consist
   of several PIO operations. However, PCI configuration space of
   virtual machine is totally emulated by the hypervisor. Cloning it
   is never a problem.

** Self-emulation
   Statistic registers exposed with attributes of read-only and
   read-clear commonly cannot be cloned through the software/hardware
   interface. The count of dropped packets in network card is an
   example. The only way to alter the count is trying to drop a
   packet. It is difficult, for it needs cooperation with the one on
   the other side of the network wire. All the existing
   solutions\cite{lm-direct-io,bonding,npia-intel} do not cover this
   register. They all do the device initialization after the
   migration, reset all statistic registers, and make the functions of
   statistic inaccurate or disabled.

   Statistic registers often have mathematical attributes. A common
   one is monotonicity. After a migration, one statistic register may
   have an incorrect value. The difference between its value and the
   right value should be a constant. For example, let's assume the
   count of dropped packets is 5 before the migration. After the
   migration, the same register on destination hardware is initialized
   to 0. After that, the value of register is always smaller than the
   right value by 5. If the value on the destination hardware is 2,
   the right value should be 7. Two packets dropped on destination
   machine and seven dropped on the source machine. In the case of
   read-clear register, the relationship is similar. The difference is
   that only the first access to a read-clear register after a
   migration may get the incorrect value.

   With such a clear logic, the classic trap-and-emulation is
   chosen. In the method of self-emulation, every access to a
   read-only or read-clear statistic register is intercepted by a
   self-emulation layer. In the layer, the right value is calculated
   and returned to the caller. The self-emulation layer can be put in
   any component on the access path of the register (e.g. the driver,
   the hypervisor). Figure \ref{fig:selfemu} represents an example
   where the self-emulation layer is in the hypervisor.

#+CAPTION: An example structure of self-emulation
#+LABEL: fig:selfemu
#+ATTR_Latex: width=2.8in
[[./selfemu.eps]]

** Choices and combination
   I/O register cloning is easy to perform, but it only works on
   states exposed by read-write registers. State replay covers almost
   every state, but it needs code changes in the driver. Statistic
   registers that are hard to clone are covered by
   self-emulation. One practical way is mixing them into a
   combination: using I/O register cloning if possible, otherwise,
   using state replay and self-emulation.

   In our case, we classifies the states of Intel 82576 virtual
   function as follows: Configurations of rings such as RDBA (Receive
   Descriptor Base Address), TXDCTL (Transmit Descriptor Control) are
   cloned using I/O register cloning. Interrupt related registers and
   settings inside Advanced Context Descriptor are cloned using state
   replay. All statistic registers are cloned using self-emulation.
   With these methods, the migration of network cards in our
   experiment runs smoothly.

* Design of CompSC
  Among the five stages of live migration\cite{lm}, CompSC works
  inside stop-and-copy stage and activation stage. Basically, CompSC
  saves states of the network device at stop-and-copy stage, and
  restores them at activation stage. The architecture of CompSC is
  presented in Figure \ref{fig:arch}.

#+CAPTION: CompSC architecture
#+LABEL: fig:arch
#+ATTR_Latex: width=3in
[[./arch.eps]]

  CompSC uses three methods to clone the device states. Before the
  migration, the driver and the hypervisor collects data using these
  methods. After the migration, the restoration of the device states
  is totally completed by the driver using collected data.

  Making the least code changes is one of CompSC's principles.
  Para-virtualized network device needs two chunks of codes working
  together to achieve the migration: One is the front-end driver, and
  the other is the back-end driver (*define in more detail*). Emulated
  network device\cite{vmware-io} has another pair, which consists of
  the emulator and the device driver. To avoid making up hundreds of
  "back-end" chunks of codes, in our solution, the hypervisor and
  virtual machine management tools do not have any device-specific
  knowledge. Everything related to the knowledge of devices is
  embedded in the network driver in the virtual machine.

** Synchronization
   As far as the device driver is concerned, device migration happens
   in a flash. After a context switch, the hardware turns into
   uninitialized state. If anything indicates the migration, it must
   be checked before any hardware access. If we define a set of
   transactions, they would never expect the disturbance of the
   migration.

   CompSC creates a shared memory area between the hypervisor and the
   virtual machine. An rwlock and a version counter are presented on
   the memory area. The rwlock indicates the status of migration. When
   the stop-and-copy stage starts, the hypervisor tries to hold the
   write lock. In the activation stage, hypervisor adds the version
   counter and releases the write lock. On the other side, the driver
   acquires the read lock before every hardware access. One the lock
   is held, the driver checks the version counter to figure out
   whether a migration has just happened. If so, the restoration of
   device driver will be invoked. In this way, the hardware will never
   be accessed in an uninitialized state.

   The logical meaning of the rwlock is the indicator of the one who
   took over the hardware device. The device driver locks the read
   lock whenever it wanted to access the hardware. After the accessing
   is finished and the device state can be taken over by the
   hypervisor for migration, the driver unlocks the read lock. The
   hypervisor acquires the write lock before it touches the
   device. When the write lock is held by the hypervisor, the hardware
   device is taken over by the hypervisor.

** I/O registers cloning
   CompSC performs the I/O register cloning easily. The hypervisor
   scans a list of registers of the network device, and saves them
   into the shared memory area. After the migration, the driver inside
   the virtual machine is responsible for restoration. To avoid having
   any device-specific knowledge, the hypervisor does not know the
   list of registers. It gets the list from the shared memory area,
   where the driver puts the list during boot process.

** State replay
   The state replay is completed in the device driver. The
   transactions and hardware operations are protected by rwlock. Every
   time before the driver releases the read lock, it stores enough
   information of the operation or transaction just finished for the
   migration. In the restoration procedure, the device drives the
   destination hardware into the same state using the saved
   information.

** Self-emulation layer
   Self-emulation layer can be put into the hypervisor or the device
   driver. A self-emulation layer in the hypervisor will trap every
   access to the emulated registers, and return the right value. A
   self-emulation layer in the driver will process the fetched value
   right after the access. The former needs less code changes in the
   driver. All it needs is the list of emulated registers, but it
   leads to performance impact due to I/O interception. The latter
   gains less overhead, but much more code changes. CompSC provides
   them both, and the driver is free to choose anyone. For the
   overhead of I/O interception, the detail will be decscribed Section
   6.1.

** SR-IOV network card
   It will be different when using SR-IOV network device. An SR-IOV
   network device consists of one PF (physical function) and several
   VFs (virtual functions). The typical usage of an SR-IOV network
   device on virtual machine is taking VFs as pass-through devices of
   virtual machines, and taking PF as a device of device domain or
   privileged domain, not only for networking, but also for VF
   management. On PCI bus, a VF looks identical to an independent PCI
   device. Also, in a virtual machine, pass-through VF is just like a
   typical PCI network card.

   VFs are managed by the PF, thus states of VFs can also be affected
   by the PF. Furthermore, some of the states can only be accessed
   through PF registers by the PF driver. When a migration happens,
   VF-in-PF states (the VF part of PF states) should also be saved and
   restored. CompSC uses the state replay method directly on the PF
   driver. The PF will record all states about the specified VF before
   the migration, and redo them on the destination machine later.

* Implementation
  We used Xen\cite{xen} as the base of our implementation. For
  architecture, we used 64-bit x86. For network card, we used Intel
  82576, an SR-IOV 1Gbps network card. The PF driver and the VF driver
  of Intel 82576 were changed in our implementation. Section 5.1
  describes the detail of driver changes, and Section 5.3 presents the
  self-emulation layer.

  Xen provides functions in the hypervisor to access foreign guest
  domain's memory page. Using these functions, shared pages between
  the hypervisor and the device driver can be well
  implemented. Section 5.2 describes the details.

  The process of live migration highly depends on dirty page
  tracking. Dirty page tracking is implementated with the help of page
  tables in the newest version of Xen. However, memory access by DMA
  could not be tracked by page tables. Intel VT-d technology\cite{vtd}
  provides I/O page tables, but it still cannot be used to track dirty
  pages. Section 5.4 discusses our solution to dirty page tracking.

** Driver changes
   Like the description in Section 4.1, the read lock of the rwlock is
   used to protect the hardware operations and the transactions we
   defined. Right after the lock is acquired, the driver checks the
   migration counter. The driver invokes restoration procedure if a
   migration just happend.

   To be specific, we packs the =igbvf_up= and =igbvf_down= as
   transactions. All the hardware operations and transactions are
   protected by the read lock. Most of device states have a copy in
   the driver, the state replay needs little code changes. The
   restoration procedure conducts the following tasks: initializing
   the device, writing all saved registers, and restoring all states
   using state replay.

** Shared page and synchronization
   Shared pages are allocated by the network device driver. The driver
   allocates several continuous pages and puts three contents into
   these pages:

   * The rwlock and the version counter;
   * The list of registers that should be saved in the migration;
   * The list of counter registers that need the help of
     self-emulation layer in the hypervisor.

   After the initialization, the GFN (guest frame number) of the first
   page is sent to the hypervisor. In our implementation, this number
   is sent by PF-VF communication. For non-SR-IOV network card, this
   number can be sent by a high level communication on TCP/IP
   protocol.

   When a live migration starts, it keeps transfering memory pages
   until the stop-and-copy stage\cite{lm}, and then tries to suspend
   the virtual machine. Right before the suspending, the write lock of
   the rwlock is acquired by the hypervisor. In this way, the
   hypervisor takes over the control of the device hardware. After the
   virtual machine is suspended, the hypervisor accesses the shared
   pages, and saves all registers listed in the shared pages. The
   remaining part of live migration happens on the backup
   machine. Before the hypervisor tries to resume the virtual machine,
   saved values of read-only and read-clear counter registers are sent
   to the self-emulation layer in the hypervisor.

   At the first time when the driver acquires the read lock, device
   restoration procedure is invoked. The driver does necessary
   initializations on the device and restores the state using
   information collected by state replay and I/O register
   cloning. After that, the device migration is accomplished
   perfectly.

** Self-emulation layer
   Xen hypervisor provides functions for trapping memory accesses. The
   self-emulation layer in the hypervisor is based on them. Every time
   the layer receives a request to commit self-emulation on a list of
   registers, it places a mark on the page table of the register. All
   the further accesses to these registers will be trapped and
   emulated. The emulation does the real MMIO, and the layer returns
   the calculated value to the virtual machine. The granularity of
   trapping in our implementation is one page. In 64-bit x86, that is
   4 KB. This may lead to unnecessary trappings and performance
   impacts. In Section 6.3, we elaborate the performance impact.

** Pages dirtied by DMA
   It is difficult to mark a page written by hardware as dirty
   automatically, while marking it manually is simple. All we need is
   doing a memory write. In a typical network device, hardware
   accesses descriptor rings and buffers by DMA. After the hardware
   wrote anyone of them, an interrupt will be sent to the driver in
   the guest kernel. The driver knows all changes on the descriptor
   rings and buffers, so it could do dummy writes (read a byte and
   write it back) to mark the pages as dirty.

   This method misses a few packets that have already been processed
   by the hardware but have not been processed by the driver yet. This
   may lead to packets duplicating or packets missing. Fortunately,
   the amount of such packets is small enough that connections of
   reliable protocols such as TCP connections would not be
   affected. Section 6.2 presents the details of these duplicated or
   missed packets.

** Descriptor ring
   During our implementation, we come across an issue on Intel 82576
   VF. The head register of descriptor rings (either RX and TX) are
   read-only. The values of them are owned by the hardware, and
   writing any value except for 0 is not allowed (writing 0 is an
   initialization). Thus, head registers can only be restored using
   state replay method.

   One method of solving it is resetting everything in the rings. By
   freeing buffers in rings and resetting rings to empty, the driver
   will work well with the device. But this method needs tens or
   hundreds of memory allocations and freeings. The time cost may be a
   problem especially when the device had a large ring.

   Another idea is shifting. Instead of restoring the value of head
   registers, we shifts the ring itself. During the restoration
   procedure, the driver shifts the RX and TX rings, and makes sure
   the position of each original head is at index 0. After that, the
   driver only needs initialization on head registers to make the
   rings work. Also, the driver saves the offsets between the original
   rings and the shifted rings. Every time the head/tail registers or
   rings are accessed by the driver, the offsets are used to make sure
   the access was right. This method introduces additional operations
   on accessing indexes/rings, so it consumes more time in the
   driver. Section 6.3 will measure this performance impact.

* Evaluation
  In this section, we present the performance data with our
  implementation of CompSC and compare them to the system without
  CompSC (original one) and the bonding driver solution. We first
  present a micro benchmark to measure the performance impact due to
  self-emulation layer in the hypervisor. Then we show our measurement
  of the number of duplicated or missed packet due to DMA dirty page
  issue in Section 6.2. With scp, netperf and SPECweb2009 benchmark,
  Section 6.3 presents a comparison of the runtime performance between
  several situations including the original environment and our
  implementation. Section 6.4 illustrates the migration process using
  a timeline figure, with CompSC, para-virtualized device, and bonding
  driver solution. In the end, Section 6.5 lists the amount of code
  changes during our implementation.

  The evaluation uses the following environment: two equivalent
  servers, with Intel Core i5 670 CPU (3.47 GHz, 4 cores), 4 GB
  memory, 1 TB harddisk, and Intel 82576 SR-IOV network card; one
  client machine for SPECweb2009 client, with Intel Core i3 540 CPU
  (3.07 GHz, 4 cores), 4 GB memory, 500 GB harddisk and an Intel
  82578DC network card. These three machines are connected using a
  1000 Mb network switcher. The virtual machine uses 4 virtual CPUs, 3
  GB memory, and a virtual function of Intel 82576 network card. It is
  virtualized in HVM (Hardware-assisted Virtual Machine). The virtual
  machine also uses a PV network device in the tests with PV device.

** Micro benchmark for self-emulation
   In Section 3.3 we present our idea of self-emulation, and figure
   out that the idea is a tradeoff between accuracy and
   performance. In this section we measure the performance loss due to
   self-emulation. In our test, we access one of the counter registers
   10,000 times. Using TSC register, we measure the total cost of CPU
   cycles and got the average. We run our test in both the
   direct-access situation and the intercepted situation. Table
   \ref{tbl:mmio} represents the results.

#+CAPTION: Micro benchmark for MMIO cost
#+LABEL: tbl:mmio
#+ATTR_Latex: align=|l|l|
   |---------------+------------------|
   | *MMIO direct* | *MMIO intercept* |
   |---------------+------------------|
   | 3911 cycles   | 11860 cycles     |
   |---------------+------------------|

   These results show that MMIO with interception needs additional
   7,949 cycles for =VMEnter/VMExit= and context switches. For low
   access frequency, this overhead is ignorable. But for high access
   frequency, the overhead may become a problem. Next, we measure the
   access frequency of statistic registers in different workloads.

#+CAPTION: Access rate of statistic registers
#+LABEL: tbl:mmio_rate
#+ATTR_Latex: align=|l|l|l|l|l|
   |---------+---------+------------+------------+--------|
   |         | *Time*  | *Rx bytes* | *Tx bytes* | *MMIO* |
   |---------+---------+------------+------------+--------|
   | Netperf | 60.02 s | 54.60 G    | 1.19 G     | 4.50/s |
   |---------+---------+------------+------------+--------|
   | SPECweb | 8015 s  | 8.55 G     | 294.68 G   | 4.50/s |
   |---------+---------+------------+------------+--------|

   Table \ref{tbl:mmio_rate} shows the access frequency of statistic
   registers. From the result, we figure out that the frequency of
   statistic register access is a constant: 4.5 access/s, no matter
   what task it was performing, and no matter which of Rx and Tx is
   heavier. A following code check on the linux kernel uncovered this
   behavior. IGBVF driver uses a watchdog with a frequency of 0.5 Hz
   to observe the statistic registers, and the access frequency is
   expected to be a constant. At such low frequency, the overhead of
   self-emulation is 10.30\us/s. With consideration of cache and TLB,
   the overhead may be slightly heavier, but it can still be
   considered small. *?*

** Duplicated and missed packet due to unmarked dirty page
   In Section 5.4, we present our idea of marking pages dirtied by
   DMA. The solution may cause packet loss and packet duplication. In
   this section, we measure the number of duplicated packets and
   missed packets under different workloads. A straight-forward
   prediction is that the number may become larger when the network
   device is busy. In our measurement, the workload of scp and SPECweb
   are used, and the situation of no workload is also considered.

#+CAPTION: Duplicated and missed packet count during live migration
#+LABEL: tbl:miss_pkt
#+ATTR_Latex: align=|l|l|l|
   |-------------+-------+--------|
   |             | *Dup* | *Miss* |
   |-------------+-------+--------|
   | No workload |     0 |      0 |
   |-------------+-------+--------|
   | scp         |     0 |      0 |
   |-------------+-------+--------|
   | SPECweb     |     0 |      3 |
   |-------------+-------+--------|

   The results in Table \ref{tbl:miss_pkt} show that, our method works
   perfectly in both no worload situation and scp situation. No packet
   loss or duplication has happened. On SPECweb workload, only 3
   packet losses have happend. These abnormal behaviors will not break
   the connection of TCP, and thus the service is kept live during the
   migration.

** Performance with workloads
   CompSC adds a synchronization method between the hypervisor and the
   driver. Performance impact of this addition is a vital data of our
   solution. The method described in Section 5.5 also has performance
   impact at runtime. In this section, the runtime performance of
   CompSC is measured and compared to original one. The self-emulation
   layer in the hypervisor also has performance overhead. Although in
   the test of Section 6.1, the overhead is measured as small, we
   still consider this factor in this section. Also, in Section 5.3 we
   describe the layer may perform unnecessary interceptions. The layer
   is optional and is only enabled after migration, so the situation
   with and without the layer are both measured.

   The first test runs a benchmark of Netperf, and an scp workload
   with a CD image file =specweb2009.iso= sized 491.72 MB. In this
   test we measure the throughput of the workload in four situations:
   Domain 0 (Dom0), original IGBVF driver (VF orig), IGBVF driver with
   CompSC (VF+comp), and IGBVF driver with CompSC and with
   self-emulation layer enabled (VF+comp+int). Figure
   \ref{fig:perf_tp} illustrates the results. In the figure, we can
   see that the throughput of four situations are almost the same in
   two workloads. Also, the CPU utilizations in the figure present
   that the VF+comp and VF+comp+int situations consume almost the same
   amount of CPU resources as VF orig situation. The CPU utilization
   of Domain 0 differs from three VF situations, because they had
   different kernel version, linux distribution, and background
   processes. The only thing we notice is that the throughput of scp
   on VF+comp+int is slightly less than that on VF orig and
   VF+comp. On Netperf benchmark, the network is the bottleneck of the
   whole system while on scp workload, CPU is the bottleneck. The CPU
   utilization near 100 percents show a CPU bottleneck of a
   single-threaded workload. The situation with self-emulation layer
   consumes more CPU resources and thus has a slightly lower
   performance.

#+CAPTION: Throughput and CPU utilization by scp and Netperf
#+LABEL: fig:perf_tp
#+ATTR_Latex: width=\linewidth
[[./perf_tp.eps]]

   SPECweb 2009 is our real-world benchmark. In our evaluation, we
   configure and run SPECweb 2009 with different pressures on the
   server in the virtual machine. We invoke the test with five
   different configurations, each with 50, 100, 150, 200, 250
   concurrent sessions respectively. Also, the tests are ran above
   three situations: original IGBVF driver (VF orig), IGBVF driver
   with CompSC (VF+comp), and IGBVF driver with CompSC and with
   self-emulation layer enabled (VF+comp+int).

   SPECweb 2009 classifies the requests based on response time into
   three types: good ones, tolerable ones, and failed ones. The good
   ones are requests which have a quick response, while the tolerable
   ones have a long but tolerable response time. Failed ones have
   intolerable response time, or no response at all. In our test, we
   collect the number of good requests and presented them in Figure
   \ref{fig:perf_spec_req}.

#+CAPTION: Good requests by SPECweb 2009
#+LABEL: fig:perf_spec_req
#+ATTR_Latex: width=\linewidth
[[./perf_spec_req.eps]]

   The number of good requests increases when the number of sessions
   is increasing linearly, until we meet the bottleneck at 250
   sessions. To uncover the bottleneck clearly, we also represent the
   average response time of requests in Figure
   \ref{fig:perf_spec_resp}. The average response times are on the
   same horizontal line when the number of sessions is less
   than 250. On the test with 250 sessions, the response time grows
   almost 2/3, indicating clearly that the server is in a heavy
   workload.

#+CAPTION: Average response time by SPECweb 2009
#+LABEL: fig:perf_spec_resp
#+ATTR_Latex: width=\linewidth
[[./perf_spec_resp.eps]]

   Before reaching the bottleneck, no obvious difference is found in
   the three situations in Figure \ref{fig:perf_spec_req} and Figure
   \ref{fig:perf_spec_resp}. This convinces that the performance
   impact of our method under light workload can be ignored. When the
   test approaches 250 sessions, VF+comp generates 3.74% fewer good
   requests than VF orig, and VF+comp+int generates 6.80% fewer good
   requests (in Figure \ref{fig:perf_spec_req}). On the measurement of
   average response time, VF+comp has 0.75% more response time and
   VF+comp+int has 2.88% more (in Figure \ref{fig:perf_spec_resp}). To
   figure out the reasons, we collect the detailed performance data
   and CPU utilization with 250 sessions in Figure
   \ref{fig:perf_spec_250}.

#+CAPTION: Performance and CPU utilization by SPECweb 2009 with 250 sessions
#+LABEL: fig:perf_spec_250
#+ATTR_Latex: width=\linewidth
[[./perf_spec_250.eps]]

#+BEGIN_LaTeX
\begin{figure*}[htb]
\epsfig{file=timeline_compsc.eps}
\caption{CompSC: Throughput and CPU utilization during live migration}
\label{fig:timeline_compsc}
\end{figure*}
#+END_LaTeX

   The total requests handled by the server in three situations are on
   the same horizontal line in Figure \ref{fig:perf_spec_250}. The
   reason why VF+comp and VF+comp+int have fewer good requests is the
   longer response time. Some of the requests are classified into
   tolerable requests because they have longer response time. In other
   words, VF+comp and VF+comp+int situation have the same service
   capability, but have slightly longer response time. In the
   meantime, VF+comp and VF+comp+int consume 0.59% and 0.64% more CPU
   respectively, whose impact can also be considered as very small.

** Service down time
#+BEGIN_LaTeX
\begin{figure*}[htb]
\epsfig{file=timeline_pv.eps}
\caption{PV device: Throughput and CPU utilization during live migration}
\label{fig:timeline_pv}
\end{figure*}
#+END_LaTex

#+BEGIN_LaTeX
\begin{figure*}[htb]
\epsfig{file=timeline_bond.eps}
\caption{Bonding driver: Throughput and CPU utilization during live migration}
\label{fig:timeline_bond}
\end{figure*}
#+END_LaTex

   In this section, we illustrate the whole process of live
   migration. We treat the server as live if it had a positive
   throughput. To fullfil the throughput, we run Netperf benchmark
   during our test. The throughput on the Netperf client machine is
   recorded as data. To shorten the migration time, which is mostly
   decided by the amount of memory, we change the virtual machine
   configuration. In this test, the virtual machine had 1 GB memory.

   During live migration, the service in the virtual machine should
   remain alive. However, in our environment, the service is stopped
   after the migration in the situations of both PV device and
   CompSC. After an analysis on network packets, we find the root
   cause. The root cause is that the network switcher does not know
   the movement of the virtual machine. It keeps forwarding packets to
   the old place of the virtual machine after the migration. We change
   both the IGBVF driver and the Xen ethernet front-end driver to send
   an ARP response after the live migration. As soon as the switcher
   receives the ARP packet, it changes its MAC-Port mapping and all
   the incoming packets are forwarded correctly.

   Figure \ref{fig:timeline_compsc} presents the throughput and CPU
   utilization during a live migration in the situation of CompSC, and
   Figure \ref{fig:timeline_pv} presents the result in the situation
   of PV device. In the figures, we first notice that the service
   downtime of CompSC is about 0.9s while the downtime of PV device is
   about 1.4s. CompSC have a 35.7% shorter and better service
   downtime. We also notice that in the test of PV device, service is
   down shortly before the 1.4s downtime (On about 20.6s). In the
   meantime, the CPU utilization goes as high as 327%. The reason of
   this behavior is the suspending process of PV-on-HVM
   (Para-virtualization on Hardware-assisted Virtual Machine). The
   suspending on PV-on-HVM needs cooperations of drivers in the
   virtual machine. These cooperations consume much CPU resources and
   cause a small period of service down. Focusing on the CPU
   utilization line, we notice that the lines on both figures have the
   same shape, and the line on Figure \ref{fig:timeline_pv} is higher
   than the line on Figure \ref{fig:timeline_compsc}. This fits our
   expectation. The pass-through device consumes less CPU resources
   than the PV device, which is the advantage of pass-through device.

   We also have a test on the solution of bonding driver. With the
   limitation of current Xen implementation, we only have a test of
   the bonding driver with a VF of Intel 82576 and an emulated E1000
   device as backup. Figure \ref{fig:timeline_bond} shows the
   result. The solution of bonding driver have an extra service down
   at about 3s. This is because the switching of bonding driver takes
   several milliseconds and causes packet loss. The shape of CPU
   utilization line is similar to that of CompSC and PV device, but
   the throughput is much less. The performance of emulated device is
   not as good as PV device or pass-through device. In the figure, we
   can also get the service downtime of bonding driver solution: about
   1.2s.

** Implementation complexity
   The CompSC needs code changes in the network device driver. In a
   common doubt on whether it is easy to be deployed, the complexity
   of device code changes is the most critical one. In Table
   \ref{tbl:loc}, we collect the line of code changes in our
   implemenation on different components. The synchronization
   mechanism is common to every network driver which is willing to do
   live migration. The total code changes of it is just 220 lines. On
   VF driver, only 183 lines of codes are added or modified. It is
   said that one can easily patch an existing device driver into a
   CompSC supported one. Even the CompSC architecture itself have
   small amount of code changes. Only 854 lines of codes are added or
   modified in both the Xen hypervisor and Xen tools. Thus, the CompSC
   is easy to deploy.

#+CAPTION: Lines of code changes in the implementation
#+LABEL: tbl:loc
#+ATTR_Latex: align=|l|l|
   |-------------------+----------------|
   |                   | *Line of code* |
   |-------------------+----------------|
   | Xen hypervisor    |            390 |
   |-------------------+----------------|
   | Xen tools         |            464 |
   |-------------------+----------------|
   | VF driver(common) |            220 |
   |-------------------+----------------|
   | VF driver(spec)   |            183 |
   |-------------------+----------------|
   | PF driver         |            181 |
   |-------------------+----------------|

* Conclusion
  In this paper we present CompSC, a state cloning mechanism to
  achieve the live migration support on pass-through network
  devices. During the migration, three kinds of device states are
  cloned using the most appropriate method. With a synchronization
  mechanism between the device driver and the hypervisor, the hardware
  is taken over by the hypervisor and performed register saving. Right
  after the migration, device driver restores the hardware state on
  the destination machine using knowledge of the device and register
  values saved by the hypervisor. Furthermore, a self-emulation layer
  inside the hypervisor is provided to achieve the accuracy of
  statistic registers.

  Our method have less than 2.88% performance impact at runtime, and a
  service downtime 35.7% shorter than that of para-virtualized network
  device during the live migration. Besides, our method needs little
  implementation effort and could be easily deployed on different
  devices.

#+LATEX: \bibliographystyle{unsrt}
#+LATEX: \bibliography{compsc}

* Comments from Middleware                                         :noexport:
  I would have liked some more results related to the robustness of
  implementation, e.g. how many times did you manage to migrate back
  and forth or in a circle around multiple random machines. Also,
  individual migration is easy, it becomes a problem in presence of
  multiple migrations taking place in the system concurrently.

  I would have liked more details on the use of migration. What
  scenarios did you use migration in, how effective it was, etc.

  you write ".. with a like count on code changes in our
  implementation, our solution is easy to deploy and can be widely
  used" This is somewhat subjective statement. First, there is a
  requirement to make code changes of the VM and hypervisor, is this
  true? This somewhat limits deployment and use.

  Please reference appropriately with number of the issue, paegs,
  year/month. (e.g. in reference 1 and elsewhere for
  magazines/journals) Please use year and pages for conference
  proceedings.

  missing discussing of the choice of benchmarks (netperf, scp), what
  kind of load the represent

  on page 1, "Several efforts have been made on enabling pass-through
  network I/O migration[15, 16, 17], but none of them solves the
  problem perfectly": the discussion on shortcomings is fragmented on
  the paper and in some cases not in sufficient depth

  refers to bonding driver in initial part of the paper without
  defining it, so discussion may be lost in he reader

  could provide better arguments on the sync part: added a lock to all
  dev operations. It's true the lock won't be contended in the common
  case, but it would make sense to discuss this when first introduced

  on page 5, replay is discussed, but without any specific information
  on when/which operations were tracked for replay? The ones related
  to read/only and read-clear?

  on page 5, "It would be different when using SR-IOV network device":
  why?

  It would be good if the paper discussed the applicability of this
  approach for other hypervisors

  The evaluation section provides table 3 with info on
  duplicated/missing packages for the scp/specweb workloads.  But what
  kind of workload could have bigger numbers?

  How come the migration setup was not working out of the box, and you
  had to find root cause and fix with the ARP?

  The paper discusses live migration support for pass-through network
  devices. How about applicability for other pass-through devices? The
  last sentence on the paper says that the method requires little
  effort and could be easily deployed on different devices (I guess
  you're assuming other network devices) but no arguments were
  provided to back this statement up

  For instance, one of the main mechanisms to support live-migration
  leverages the replay of past communication to bring the new driver
  to the up-to-date state. A natural question to me seems how scalable
  is this solution. How much state need to be saved to enable state
  replay? The authors mention that some optimizations are possible but
  it would have been nice to see some real numbers and experimental
  analysis.

  both CompSC and the para-virtualized driver achieve the same
  throughput. I can imagine that under more challenging scenarios
  (e.g., a 10-Gbps network interface), the direct I/O driver would
  outperform the para-virtualized one. It would be interesting to see
  in these conditions what are the performance of CompSC.
